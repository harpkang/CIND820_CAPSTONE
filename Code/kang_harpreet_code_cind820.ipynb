{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #2F4F4F;\">Author: Harpreet Kang</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Capstone Project: CIND820 \n",
    "Harpreet Kang\n",
    "July 2, 2024\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #2F4F4F;\">Data Load </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <span style=\"color: #4682B4;\">Loading the Libraries</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from ydata_profiling import ProfileReport\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV,KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error,precision_score,recall_score,f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "from scipy import stats\n",
    "from scipy.stats import friedmanchisquare,kruskal\n",
    "import statsmodels.api as sm\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\n",
    "from xgboost import XGBRegressor,XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.svm import SVR,SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Displaying all the columns and rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\">Loading the Data</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('dataset_lfs_2024.xlsx')\n",
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #2F4F4F;\">Pre-Processing </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\">Filtering the Data and Fixing Decimal Places</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[\"HRLYEARN\"],inplace=True)\n",
    "# Removing the null values in HRLYEARN column\n",
    "data=data[data['MJH']==1]\n",
    "# Filtering for single job holders\n",
    "data=data[data['SCHOOLN']==1]\n",
    "# Filtering for non students\n",
    "data=data[data['FTPTMAIN']==1]\n",
    "# Filtering for full-time workers\n",
    "data=data[data['PERMTEMP']==1]\n",
    "# Filtering for permanent workers\n",
    "data['HRLYEARN']=data['HRLYEARN']/100\n",
    "# The data dictionary for this dataset indicated that the last 2 values of this numeric column were the decimal points. So dividing the HRLYEARN column by 100 will add 2 decimal points. \n",
    "data['UHRSMAIN']=data['UHRSMAIN']/10\n",
    "data['AHRSMAIN']=data['AHRSMAIN']/10\n",
    "data['UTOTHRS']=data['UTOTHRS']/10\n",
    "data['ATOTHRS']=data['ATOTHRS']/10\n",
    "data['HRSAWAY']=data['HRSAWAY']/10\n",
    "data['PAIDOT']=data['PAIDOT']/10\n",
    "data['UNPAIDOT']=data['UNPAIDOT']/10\n",
    "data['XTRAHRS']=data['XTRAHRS']/10\n",
    "# The data dictionary for this dataset indicated that the last 1 decimial of the 8 numeric columns above were the decimal points\n",
    "# So dividing the UHRSMAIN column by 00 will add 1 decimal points.\n",
    "# For HRLYEARN, it was the last two decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile1 = ProfileReport(data, title=\"Profiling Report\")\n",
    "#profile1.to_file(\"EDA after eliminating blanks of Usual Hourly Wages (HRLYEARN).html\")\n",
    "# Generating the profiling report after removing the null values in HRLYEARN column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: #4682B4;\">Mapping Variables to Discriptions</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_variable_name_mapping={\n",
    "'rec_num':'Order of record in file',\n",
    "'survyear':'Survey year',\n",
    "'survmnth':'Survey month',\n",
    "'lfsstat':'Labour force status',\n",
    "'prov':'Province',\n",
    "'cma':'Nine largest CMAs',\n",
    "'age_12':'Five-year age group of respondent',\n",
    "'age_6':'Age in 2 and 3 year groups, 15 to 29',\n",
    "'sex':'Sex of respondent',\n",
    "'marstat':'Marital status of respondent',\n",
    "'educ':'Highest educational attainment',\n",
    "'mjh':'Single or multiple jobholder',\n",
    "'everwork':'Identifies if a person has worked in the last year',\n",
    "'ftptlast':'Full- or part-time status of last job',\n",
    "'cowmain':'Class of worker, main job',\n",
    "'immig':'Immigrant status',\n",
    "'naics_21':'Industry of main job',\n",
    "'noc_10':'Occupation at main job (noc_10)',\n",
    "'noc_43':'Occupation at main job (noc_43)',\n",
    "'yabsent':'Reason of absence, full week',\n",
    "'wksaway':'Number of weeks absent from work',\n",
    "'payaway':'Paid for time off, full-week absence only',\n",
    "'uhrsmain':'Usual hours worked per week at main job',\n",
    "'ahrsmain':'Actual hours worked per week at main job',\n",
    "'ftptmain':'Full- or part-time status at main or only job',\n",
    "'utothrs':'Usual hours worked per week at all jobs',\n",
    "'atothrs':'Actual hours worked per week at all jobs',\n",
    "'hrsaway':'Hours away from work, part-week absence only',\n",
    "'yaway':'Reason for part-week absence',\n",
    "'paidot':'Paid overtime hours in reference week',\n",
    "'unpaidot':'Unpaid overtime hours in reference week',\n",
    "'xtrahrs':'Number of overtime or extra hours worked',\n",
    "'whypt':'Reason for part-time work',\n",
    "'tenure':'Job tenure with current employer',\n",
    "'prevten':'Job tenure with previous employer',\n",
    "'hrlyearn':'Usual hourly wages',\n",
    "'union':'Union status',\n",
    "'permtemp':'Job permanency',\n",
    "'estsize':'Establishment size',\n",
    "'firmsize':'Firm size',\n",
    "'durunemp':'Duration of unemployment',\n",
    "'flowunem':'Flows into unemployment',\n",
    "'unemftpt':'Job seekers by type of work sought and temporary layoffs by work status of last job',\n",
    "'whylefto':'Reason for leaving job during previous year (whylefto)',\n",
    "'whyleftn':'Reason for leaving job during previous year (whyleftn)',\n",
    "'durjless':'Duration of joblessness',\n",
    "'availabl':'Availability during the reference week',\n",
    "'lkpubag':'Unemployed, used public employment agency',\n",
    "'lkemploy':'Unemployed, checked with employers directly',\n",
    "'lkrels':'Unemployed, checked with friends or relatives',\n",
    "'lkatads':'Unemployed, looked at job ads',\n",
    "'lkansads':'Unemployed, placed or answered ads',\n",
    "'lkothern':'Unemployed, other methods',\n",
    "'prioract':'Main activity before started looking for work',\n",
    "'ynolook':'Reason for not looking for work during the reference week',\n",
    "'tlolook':'Temporary layoff, looked for work during the last four weeks',\n",
    "'schooln':'Current student status',\n",
    "'efamtype':'Type of economic family',\n",
    "'agyownk':'Age of youngest child',\n",
    "'finalwt':'Standard final weight'}\n",
    "\n",
    "# Mapping descriptions to the variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.describe()\n",
    "# Displaying the summary statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.info()\n",
    "# Displaying the data types of the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Calculating Missing Values in the Dataset</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_missing_values= data.isnull().sum()\n",
    "# Counting the missing values in each column\n",
    "data_percent_missing = round(data.isnull().sum() * 100 / len(data),2)\n",
    "# Calculating the percentage of missing values in each column\n",
    "missing_values_percent=pd.DataFrame(data_percent_missing, columns=['Missing Values %'])\n",
    "# Creating a dataframe to display the missing values percentage\n",
    "missing_values_percent=missing_values_percent.rename_axis('Variables')\n",
    "# Renaming the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()\n",
    "# Displaying the first 5 rows of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Creating a Function to generate Descriptive Statistics</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_table(data_frame):\n",
    "    data_type_mapping={\n",
    "    'rec_num':'Nominal',\n",
    "    'survyear':'Ordinal',\n",
    "    'survmnth':'Ordinal',\n",
    "    'lfsstat':'Nominal',\n",
    "    'prov':'Nominal',\n",
    "    'cma':'Nominal',\n",
    "    'age_12':'Ordinal',\n",
    "    'age_6':'Ordinal',\n",
    "    'sex':'Nominal',\n",
    "    'marstat':'Nominal',\n",
    "    'educ':'Ordinal',\n",
    "    'mjh':'Nominal',\n",
    "    'everwork':'Nominal',\n",
    "    'ftptlast':'Nominal',\n",
    "    'cowmain':'Nominal',\n",
    "    'immig':'Nominal',\n",
    "    'naics_21':'Nominal',\n",
    "    'noc_10':'Nominal',\n",
    "    'noc_43':'Nominal',\n",
    "    'yabsent':'Nominal',\n",
    "    'wksaway':'Nominal',\n",
    "    'payaway':'Nominal',\n",
    "    'uhrsmain':'Continuous',\n",
    "    'ahrsmain':'Continuous',\n",
    "    'ftptmain':'Nominal',\n",
    "    'utothrs':'Continuous',\n",
    "    'atothrs':'Continuous',\n",
    "    'hrsaway':'Continuous',\n",
    "    'yaway':'Nominal',\n",
    "    'paidot':'Continuous',\n",
    "    'unpaidot':'Continuous',\n",
    "    'xtrahrs':'Continuous',\n",
    "    'whypt':'Nominal',\n",
    "    'tenure':'Discrete',\n",
    "    'prevten':'Discrete',\n",
    "    'hrlyearn':'Continuous',\n",
    "    'union':'Nominal',\n",
    "    'permtemp':'Nominal',\n",
    "    'estsize':'Ordinal',\n",
    "    'firmsize':'Ordinal',\n",
    "    'durunemp':'Discrete',\n",
    "    'flowunem':'Nominal',\n",
    "    'unemftpt':'Nominal',\n",
    "    'whylefto':'Nominal',\n",
    "    'whyleftn':'Nominal',\n",
    "    'durjless':'Discrete',\n",
    "    'availabl':'Nominal',\n",
    "    'lkpubag':'Nominal',\n",
    "    'lkemploy':'Nominal',\n",
    "    'lkrels':'Nominal',\n",
    "    'lkatads':'Nominal',\n",
    "    'lkansads':'Nominal',\n",
    "    'lkothern':'Nominal',\n",
    "    'prioract':'Nominal',\n",
    "    'ynolook':'Nominal',\n",
    "    'tlolook':'Nominal',\n",
    "    'schooln':'Nominal',\n",
    "    'efamtype':'Nominal',\n",
    "    'agyownk':'Ordinal',\n",
    "    'finalwt':'Continuous'}\n",
    "\n",
    "    data_object_mapping={\n",
    "    'rec_num':'Qualitative',\n",
    "    'survyear':'Qualitative',\n",
    "    'survmnth':'Qualitative',\n",
    "    'lfsstat':'Qualitative',\n",
    "    'prov':'Qualitative',\n",
    "    'cma':'Qualitative',\n",
    "    'age_12':'Qualitative',\n",
    "    'age_6':'Qualitative',\n",
    "    'sex':'Qualitative',\n",
    "    'marstat':'Qualitative',\n",
    "    'educ':'Qualitative',\n",
    "    'mjh':'Qualitative',\n",
    "    'everwork':'Qualitative',\n",
    "    'ftptlast':'Qualitative',\n",
    "    'cowmain':'Qualitative',\n",
    "    'immig':'Qualitative',\n",
    "    'naics_21':'Qualitative',\n",
    "    'noc_10':'Qualitative',\n",
    "    'noc_43':'Qualitative',\n",
    "    'yabsent':'Qualitative',\n",
    "    'wksaway':'Qualitative',\n",
    "    'payaway':'Qualitative',\n",
    "    'uhrsmain':'Quantitative',\n",
    "    'ahrsmain':'Quantitative',\n",
    "    'ftptmain':'Qualitative',\n",
    "    'utothrs':'Quantitative',\n",
    "    'atothrs':'Quantitative',\n",
    "    'hrsaway':'Quantitative',\n",
    "    'yaway':'Qualitative',\n",
    "    'paidot':'Quantitative',\n",
    "    'unpaidot':'Quantitative',\n",
    "    'xtrahrs':'Quantitative',\n",
    "    'whypt':'Qualitative',\n",
    "    'tenure':'Quantitative',\n",
    "    'prevten':'Quantitative',\n",
    "    'hrlyearn':'Quantitative',\n",
    "    'union':'Qualitative',\n",
    "    'permtemp':'Qualitative',\n",
    "    'estsize':'Qualitative',\n",
    "    'firmsize':'Qualitative',\n",
    "    'durunemp':'Qualitative',\n",
    "    'flowunem':'Qualitative',\n",
    "    'unemftpt':'Qualitative',\n",
    "    'whylefto':'Qualitative',\n",
    "    'whyleftn':'Qualitative',\n",
    "    'durjless':'Qualitative',\n",
    "    'availabl':'Qualitative',\n",
    "    'lkpubag':'Qualitative',\n",
    "    'lkemploy':'Qualitative',\n",
    "    'lkrels':'Qualitative',\n",
    "    'lkatads':'Qualitative',\n",
    "    'lkansads':'Qualitative',\n",
    "    'lkothern':'Qualitative',\n",
    "    'prioract':'Qualitative',\n",
    "    'ynolook':'Qualitative',\n",
    "    'tlolook':'Qualitative',\n",
    "    'schooln':'Qualitative',\n",
    "    'efamtype':'Qualitative',\n",
    "    'agyownk':'Qualitative',\n",
    "    'finalwt':'Quantitative'}\n",
    "    data_table=data_frame.columns.to_frame(index=False)\n",
    "    data_table=data_table.rename(columns={0:'Variable Name'})\n",
    "    columns=list(data_table.iloc[:,0] )\n",
    "    data_table_mean_lst=[]\n",
    "    data_table_median_lst=[]\n",
    "    data_table_min_lst=[]\n",
    "    data_table_max_lst=[]\n",
    "    data_table_std_lst=[]\n",
    "    data_table_object_lst=[]\n",
    "    data_table_dtype_lst=[]\n",
    "\n",
    "    for column in columns:\n",
    "        data_table_mean_lst.append(round(data_frame[column].mean(),2))\n",
    "        data_table_median_lst.append(round(data_frame[column].median(),2))\n",
    "        data_table_min_lst.append(round(data_frame[column].min(),2))\n",
    "        data_table_max_lst.append(round(data_frame[column].max(),2))\n",
    "        data_table_std_lst.append(round(data_frame[column].std(),2))\n",
    "        data_table_object_lst.append(data_object_mapping.get((column.lower())))\n",
    "        data_table_dtype_lst.append(data_type_mapping.get((column.lower())))\n",
    "    summary_statistics_table=pd.DataFrame(list(zip(columns,data_table_object_lst,data_table_dtype_lst,data_table_mean_lst,data_table_median_lst,data_table_min_lst,data_table_max_lst,data_table_std_lst)),\n",
    "                                      columns=['Variable','Data','Data Type','Mean','Median','Min','Max','Standard Deviation'])\n",
    "    return summary_statistics_table\n",
    "\n",
    "# Created a function to generate the summary statistics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics_table=statistics_table(data)\n",
    "# Creating the summary statistics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing mean, and standard deviation from non continous and discrete variables\n",
    "summary_statistics_table[\"Mean\"] = summary_statistics_table.apply(lambda row: row[\"Mean\"] if row[\"Data Type\"].lower() in [\"continuous\", \"discrete\"] else None, axis=1)\n",
    "summary_statistics_table[\"Standard Deviation\"] = summary_statistics_table.apply(lambda row: row[\"Standard Deviation\"] if row[\"Data Type\"].lower() in [\"continuous\", \"discrete\"] else None, axis=1)\n",
    "\n",
    "# Removing the median from the nominal variables\n",
    "summary_statistics_table[\"Median\"] = summary_statistics_table.apply(lambda row: None if row[\"Data Type\"].lower() == \"nominal\" else row[\"Median\"] , axis=1)\n",
    "summary_statistics_table=summary_statistics_table.merge(missing_values_percent,left_on='Variable',right_index=True)\n",
    "summary_statistics_table.sort_values(by=['Missing Values %','Variable'],inplace=True)\n",
    "summary_statistics_table.reset_index(drop=True,inplace=True)\n",
    "\n",
    "summary_statistics_table_1=summary_statistics_table[['Variable','Data','Data Type']]\n",
    "summary_statistics_table_1[\"Description\"]=summary_statistics_table_1[\"Variable\"].apply(lambda x: full_variable_name_mapping.get(x.lower()))\n",
    "summary_statistics_table_1=summary_statistics_table_1[['Variable','Description','Data','Data Type']]\n",
    "#summary_statistics_table_1.to_excel('Summary Statistics Table_Figure_3.xlsx',index=False)  \n",
    "\n",
    "summary_statistics_table_2=summary_statistics_table.drop(columns=['Data','Data Type'],axis=1)\n",
    "#summary_statistics_table_2.to_excel('Summary Statistics Table_Figure_2.xlsx',index=False)  \n",
    "\n",
    "# Creating a function to generate the statistics table for excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #BDB76B;\">Feature Selection: Low Variance </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_attributes=list(summary_statistics_table[summary_statistics_table_2['Missing Values %']>56.00]['Variable'])\n",
    "# Creating a list of variables with more than 56% missing values\n",
    "\n",
    "# Dropping the below variables as they are not important\n",
    "remove_attributes.append('SURVMNTH')\n",
    "remove_attributes.append('SURVYEAR')\n",
    "remove_attributes.append('REC_NUM')\n",
    "remove_attributes.append('FINALWT')\n",
    "\n",
    "##################################\n",
    "\n",
    "remove_attributes.append('LFSSTAT')\n",
    "# Already filtered for employed\n",
    "remove_attributes.append('MJH')\n",
    "# Only looking at single job holders\n",
    "remove_attributes.append('FTPTMAIN')\n",
    "# Deleting because we are only looking at full timer workers not parttime\n",
    "remove_attributes.append('PERMTEMP')\n",
    "# Already filtered for full time jobs\n",
    "remove_attributes.append('SCHOOLN')\n",
    "# Only looking at non students and not full time or part time students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #BDB76B;\">Feature Selection: Missing Values </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_attributes.append('HRSAWAY')\n",
    "# Hours away from work, part-week absence only low variance has 79 perecent 0s\n",
    "remove_attributes.append('PAIDOT')\n",
    "# Paid over time has low variance 83% of zeros\n",
    "remove_attributes.append('UNPAIDOT')\n",
    "# Upaid overtime has 83% of zeros\n",
    "remove_attributes.append('XTRAHRS')\n",
    "# Contains over 70% of zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data.drop(columns=remove_attributes)\n",
    "# dropping the variables with more than 56% missing values and creating a new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Mapping all of the Categorical Variables from the Data Dictionary</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['SEX']=data2['SEX'].map({1:0,2:1})\n",
    "# Male is 0 and Female is 1. \n",
    "data2['IMMIG']=data2['IMMIG'].map({1:1,2:1,3:0})\n",
    "# immigrant is 1 and non immigrant is 0data2['MARSTAT']=data2['MARSTAT'].map({1:1,2:0,3:0,4:0,5:0,6:0})\n",
    "data2['MARSTAT']=data2['MARSTAT'].map({1:1,2:0,3:0,4:0,5:0,6:0})\n",
    "# Married is 1 and not married is 0\n",
    "data2['CMA']=data2['CMA'].map({0:0,1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:1})\n",
    "# 1 is for the 9 largest CMAs and 0 is for the rest.\n",
    "\n",
    "data2['EFAMTYPE']=data2['EFAMTYPE'].map({\n",
    "    1:'Person not in an economic family',\n",
    "    2:'Dual-earner couple, no children or none under 25',\n",
    "    3:'Dual-earner couple, youngest child 0 to 17',\n",
    "    4:'Dual-earner couple, youngest child 18 to 24',\n",
    "    5:'Single-earner couple, male employed, no children or none under 25',\n",
    "    6:'Single-earner couple, male employed, youngest child 0 to 17',\n",
    "    7:'Single-earner couple, male employed, youngest child 18 to 24',\n",
    "    8:'Single-earner couple, female employed, no children or none under 25',\n",
    "    9:'Single-earner couple, female employed, youngest child 0 to 17',\n",
    "    10:'Single-earner couple, female employed, youngest child 18 to 24',\n",
    "    11:'Non-earner couple, no children or none under 25',\n",
    "    12:'Non-earner couple, youngest child 0 to 17',\n",
    "    13:'Non-earner couple, youngest child 18 to 24',\n",
    "    14:'Lone-parent family, parent employed, youngest child 0 to 17',\n",
    "    15:'Lone-parent family, parent employed, youngest child 18 to 24',\n",
    "    16:'Lone-parent family, parent not employed, youngest child 0 to 17',\n",
    "    17:'Lone-parent family, parent not employed, youngest child 18 to 24',\n",
    "    18:'Other families'\n",
    "})\n",
    "# Mapping the economic family type\n",
    "\n",
    "data2['COWMAIN']=data2['COWMAIN'].map({1:'Public sector employees',2:'Private sector employees'})\n",
    "# Mapping the class of worker\n",
    "\n",
    "data2['PROV']=data2['PROV'].map({10:'NL',11:'PE',12:'NS',13:'NB',24:'QC',35:'ON',46:'MB',47:'SK',48:'AB',59:'BC'})\n",
    "# Mapping the provinces\n",
    "\n",
    "data2['AGE_12']=data2['AGE_12'].map({\n",
    "    1:'15-19'\n",
    "    ,2:'20-24'\n",
    "    ,3:'25-29'\n",
    "    ,4:'30-34'\n",
    "    ,5:'35-39'\n",
    "    ,6:'40-44'\n",
    "    ,7:'45-49'\n",
    "    ,8:'50-54'\n",
    "    ,9:'55-59'\n",
    "    ,10:'60-64'\n",
    "    ,11:'65-69'\n",
    "    ,12:'70+'})\n",
    "# Mapping the age groups\n",
    "\n",
    "data2['EDUC']=data2['EDUC'].map({\n",
    "    0:'0 to 8 years'\n",
    "    ,1:'Some high school'\n",
    "    ,2:'High school graduate'\n",
    "    ,3:'Some post-secondary'\n",
    "    ,4:'Post-secondary certificate or diploma'\n",
    "    ,5:'Bachelor\\'s degree'\n",
    "    ,6:'Above bachelor\\'s degree'})\n",
    "# Mapping the education levels\n",
    "\n",
    "data2['NAICS_21']=data2['NAICS_21'].map({\n",
    "1:'Agriculture'\n",
    ",2:'Forestry and logging and support activities for forestry'\n",
    ",3:'Fishing, hunting and trapping'\n",
    ",4:'Mining, quarrying, and oil and gas extraction'\n",
    ",5:'Utilities'\n",
    ",6:'Construction'\n",
    ",7:'Manufacturing - durable goods'\n",
    ",8:'Manufacturing - non-durable goods'\n",
    ",9:'Wholesale trade'\n",
    ",10:'Retail trade'\n",
    ",11:'Transportation and warehousing'\n",
    ",12:'Finance and insurance'\n",
    ",13:'Real estate and rental and leasing'\n",
    ",14:'Professional, scientific and technical services'\n",
    ",15:'Business, building and other support services'\n",
    ",16:'Educational services'\n",
    ",17:'Health care and social assistance'\n",
    ",18:'Information, culture and recreation'\n",
    ",19:'Accommodation and food services'\n",
    ",20:'Other services (except public administration)'\n",
    ",21:'Public administration' \n",
    "})\n",
    "# Mapping the industry\n",
    "\n",
    "data2['FIRMSIZE']=data2['FIRMSIZE'].map({\n",
    "     1:'Less than 20 employees'\n",
    "    ,2:'20-99 employees'\n",
    "    ,3:'100-500 employees'\n",
    "    ,4:'More than 500 employees'})\n",
    "# Mapping the firm size\n",
    "\n",
    "data2['UNION']=data2['UNION'].map({\n",
    "    1:'Union member',\n",
    "    2:'Not a member but covered by a union contract or collective agreement',\n",
    "    3:'Non-unionized'})\n",
    "# Mapping the union status\n",
    "\n",
    "data2['NOC_43']=data2['NOC_43'].map({\n",
    "    1:'Legislative and senior management occupations',\n",
    "    2:'Specialized middle management occupations',\n",
    "    3:'Middle management occupations in retail and wholesale trade and customer services',\n",
    "    4:'Middle management occupations in trades, transportation, production and utilities',\n",
    "    5:'Professional occupations in finance',\n",
    "    6:'Professional occupations in business',\n",
    "    7:'Administrative and financial supervisors and specialized administrative occupations',\n",
    "    8:'Administrative occupations and transportation logistics occupations',\n",
    "    9:'Administrative and financial support and supply chain logistics occupations',\n",
    "    10:'Professional occupations in natural sciences',\n",
    "    11:'Professional occupations in applied sciences (except engineering)',\n",
    "    12:'Professional occupations in engineering',\n",
    "    13:'Technical occupations related to natural and applied sciences',\n",
    "    14:'Health treating and consultation services professionals',\n",
    "    15:'Therapy and assessment professionals',\n",
    "    16:'Nursing and allied health professionals',\n",
    "    17:'Technical occupations in health',\n",
    "    18:'Assisting occupations in support of health services',\n",
    "    19:'Professional occupations in law',\n",
    "    20:'Professional occupations in education services',\n",
    "    21:'Professional occupations in social and community services',\n",
    "    22:'Professional occupations in government services',\n",
    "    23:'Occupations in front-line public protection services',\n",
    "    24:'Paraprofessional occupations in legal, social, community and education services',\n",
    "    25:'Assisting occupations in education and in legal and public protection',\n",
    "    26:'Care providers and public protection support occupations and student monitors, crossing guards and related occupations',\n",
    "    27:'Professional occupations in art and culture',\n",
    "    28:'Technical occupations in art, culture and sport',\n",
    "    29:'Occupations in art, culture and sport',\n",
    "    30:'Support occupations in art, culture and sport',\n",
    "    31:'Retail sales and service supervisors and specialized occupations in sales and services',\n",
    "    32:'Occupations in sales and services',\n",
    "    33:'Sales and service representatives and other customer and personal services occupations',\n",
    "    34:'Sales and service support occupations',\n",
    "    35:'Technical trades and transportation officers and controllers',\n",
    "    36:'General trades',\n",
    "    37:'Mail and message distribution, other transport equipment operators and related maintenance workers',\n",
    "    38:'Helpers and labourers and other transport drivers, operators and labourers',\n",
    "    39:'Supervisors and occupations in natural resources, agriculture and related production',\n",
    "    40:'Workers and labourers in natural resources, agriculture and related production',\n",
    "    41:'Supervisors, central control and process operators in processing, manufacturing and utilities and aircraft assemblers and inspectors',\n",
    "    42:'Machine operators, assemblers and inspectors in processing, manufacturing and printing',\n",
    "    43:'Labourers in processing, manufacturing and utilities'\n",
    "})\n",
    "# Mapping the occupation\n",
    "\n",
    "data2['NOC_10']=data2['NOC_10'].map({\n",
    "    1:'Management occupations',\n",
    "    2:'Business, finance and administration occupations, except management',\n",
    "    3:'Natural and applied sciences and related occupations, except management',\n",
    "    4:'Health occupations, except management',\n",
    "    5:'Occupations in education, law and social, community and government services, except management',\n",
    "    6:'Occupations in art, culture, recreation and sport, except management',\n",
    "    7:'Sales and service occupations, except management',\n",
    "    8:'Trades, transport and equipment operators and related occupations, except management',\n",
    "    9:'Natural resources, agriculture and related production occupations, except management',\n",
    "    10:'Occupations in manufacturing and utilities, except management'})\n",
    "# Mapping the occupation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #8FBC8F;\">Saving Checkpoint 1</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=data2.copy()\n",
    "# Creating a copy of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Splitting the Data into Training and Testing</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=data3['HRLYEARN']\n",
    "#Creating the label for splitting the data\n",
    "\n",
    "features=data3.drop(columns=['HRLYEARN'],axis=1)\n",
    "# Creating the features for splitting the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, train_size=.7, random_state=42)\n",
    "# Splitting the features and labels into training and testing datasets\n",
    "# The training dataset is 80% of the data and the testing dataset is 20% of the data\n",
    "# This is to ensure no data leekage occurs, and to ensure the model is not overfitting\n",
    "# X_train will be used as the training set and the validation set for machine learning later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #BDB76B;\">Feature Selection: Correlation Analysis</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile2 = ProfileReport(X_train, title=\"Profiling Report\")\n",
    "#profile2.to_file(\"EDA after removing non-correlated features.html\")\n",
    "# Creating a another profile report to examine the correlation just on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will be dropping the remaining measures becauase of high correlation with other variables\n",
    "# This is removed from the X_train variable\n",
    "\n",
    "remove_attributes2=[]\n",
    "# Creating an empty list to store the variables to be removed\n",
    "remove_attributes2.append('NOC_43')\n",
    "# Correlated with COWMAIN, NOCS_10, SEX\n",
    "remove_attributes2.append('NOC_10')\n",
    "# Correlated with SEX, NOC_43, NAICS_21\n",
    "remove_attributes2.append('ESTSIZE')\n",
    "# Dropping MJH because it only contains one value, so it is not useful for the model\n",
    "remove_attributes2.append('AHRSMAIN')\n",
    "# Correlated with UTOTHRS, UHRSMAIN, ATOTHRS\n",
    "remove_attributes2.append('UTOTHRS')\n",
    "# Correlated with  ATOTHRS, UHRSMAIN, AHRSMAIN\n",
    "remove_attributes2.append('ATOTHRS')\n",
    "# Correlated with UTOTHRS, UHRSMAIN, ATOTHRS\n",
    "remove_attributes2.append('COWMAIN')\n",
    "# Removing cowmain because it is correlated with NAICS_21, UNION and NOC_43\n",
    "remove_attributes2.append('EFAMTYPE')\n",
    "# Removing efamtype because it is correlated with MASRSTAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.drop(columns=remove_attributes2)\n",
    "X_test=X_test.drop(columns=remove_attributes2)\n",
    "# Dropping the variables with high correlation in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile3 = ProfileReport(X_train, title=\"Profiling Report\")\n",
    "#profile3.to_file(\"EDA after removing the correlated features.html\")\n",
    "# Dropping the variables with a high correlatoion and making another profile report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list_of_variables_removed=remove_attributes+remove_attributes2\n",
    "null_attributes_full_variable_name=[]\n",
    "for attribute in master_list_of_variables_removed:\n",
    "    null_attributes_full_variable_name.append(full_variable_name_mapping.get(attribute.lower()))\n",
    "# Compiling all the variables that were removed from the dataset\n",
    "\n",
    "null_attributes_table=pd.DataFrame({'Variable':master_list_of_variables_removed,'Description':null_attributes_full_variable_name})\n",
    "# Creating a null attributes table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Combining the X_train and y_train into one Dataframe and X_test and y_test</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=X_train.join(y_train, how='inner')\n",
    "\n",
    "print(data3.info())\n",
    "# Joing back the training labels to the training set to have a complete dataset and make it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=X_test.join(y_test,how='inner')\n",
    "# Joing back the test labels to the test set to have a complete dataset and make it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe with the all the unique values in data3 and the number of classes in the categorical columns\n",
    "unique_values_table=pd.DataFrame()\n",
    "for column in data3.columns:\n",
    "    unique_values_table[column]=[data3[column].nunique()]\n",
    "unique_values_table=unique_values_table.T\n",
    "unique_values_table=unique_values_table.rename(columns={0:'Unique Values'})\n",
    "unique_values_table=unique_values_table.reset_index()\n",
    "unique_values_table=unique_values_table.rename(columns={'index':'Variables'})\n",
    "unique_values_table=unique_values_table.sort_values(by='Unique Values',ascending=False)\n",
    "unique_values_table.reset_index(drop=True,inplace=True)\n",
    "#unique_values_table.to_excel('Unique Values Table_Figure_4.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #8FBC8F;\">Saving Checkpoint 2</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4=data3.copy()\n",
    "#Creating a checkpoint for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Creating Dummy Variables and Scaling</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Renaming Age_12 and NAICS_21 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.rename(columns={'AGE_12': 'AGE'}, inplace=True)\n",
    "# Renaming the age column\n",
    "data4.rename(columns={'NAICS_21':'NAICS'},inplace=True)\n",
    "# Renaming the age column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OneHotEncoder and MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['PROV', 'CMA', 'AGE', 'SEX', 'MARSTAT', 'EDUC', 'IMMIG', 'NAICS', 'UNION', 'FIRMSIZE']\n",
    "continuous_features = ['UHRSMAIN', 'TENURE']\n",
    "\n",
    "y = data4['HRLYEARN']\n",
    "X = data4.drop(columns=['HRLYEARN'])\n",
    "\n",
    "# Settting up the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), continuous_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Transforming X\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Checking the shape of the X\n",
    "#print(\"Shape of X:\", X_transformed.shape)\n",
    "\n",
    "# Get feature names after transformation\n",
    "categorical_transformer = preprocessor.named_transformers_['cat']\n",
    "onehot_feature_names = categorical_transformer.get_feature_names_out(categorical_features)\n",
    "feature_names = np.append(continuous_features, onehot_feature_names)\n",
    "\n",
    "# Counting of feature names\n",
    "#print(\"Number of feature names:\", len(feature_names))\n",
    "\n",
    "# Converting tge sparse matrix to a dense matrix\n",
    "X_transformed_dense = X_transformed.toarray()\n",
    "\n",
    "# Output the transformed features with the original index\n",
    "features = pd.DataFrame(X_transformed_dense, columns=feature_names, index=data4.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(features.shape)\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Random Forest Graph of Feature Importance</span>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Parameter Grid\n",
    "param_grid = {\n",
    "    'model__n_estimators': [10],\n",
    "    'model__max_features': [1.0],\n",
    "    'model__max_depth': [None],\n",
    "    'model__min_samples_split': [2],\n",
    "    'model__min_samples_leaf': [1],\n",
    "    'model__bootstrap': [True]\n",
    "}\n",
    "# KFolds\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# RamdomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid,n_iter=1,cv=kf,random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X, y)\n",
    "\n",
    "my_model = random_search.best_estimator_\n",
    "\n",
    "# Getting the model trained\n",
    "model = my_model.named_steps['model']\n",
    "\n",
    "# Retreiving the feature names\n",
    "categorical_transformer = my_model.named_steps['preprocessor'].named_transformers_['cat']\n",
    "onehot_feature_names = categorical_transformer.get_feature_names_out(categorical_features)\n",
    "feature_names = np.append(continuous_features, onehot_feature_names)\n",
    "\n",
    "# Extracted the feature importances from the model\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Created a DataFrame for better visualization\n",
    "features_data_frame = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "features_data_frame = features_data_frame.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=features_data_frame, palette='viridis')\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Checking for Normality</span>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Histograms of the Quantititaive Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histogram(feature_series, titles, x_labels):\n",
    "    # Create subplots\n",
    "    fig, ax = plt.subplots(ncols=len(feature_series), nrows=1, figsize=(20, 7))\n",
    "\n",
    "    # Plot each feature in a separate subplot\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        try:\n",
    "            axi.hist(feature_series[i], bins=50, color='skyblue', edgecolor='black')\n",
    "            axi.set_title(titles[i], fontsize=14)\n",
    "            axi.set_xlabel(x_labels[i], fontsize=12)\n",
    "            axi.set_ylabel('Frequency', fontsize=12)\n",
    "        except IndexError:\n",
    "            break\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_histogram([data4['HRLYEARN'], data4['UHRSMAIN'], data4['TENURE']],['Hourly Earnings (in dollars)', 'Usual Hours Worked (per week)', 'Job Tenure (in months)'], ['Dollars', 'Hours', 'Months'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shaprio-Wilk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Shapiro-Wilk test to test for normality for UHRSMAIN and TENURE\n",
    "for column in ['UHRSMAIN','TENURE']:\n",
    "    stat, p = stats.shapiro(data4[column])\n",
    "    #print(f'Statistic: {stat}, p-value: {p}')\n",
    "    pass\n",
    "\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        #print(f'{column} looks like a normal distribution (fail to reject H0)')\n",
    "        pass\n",
    "    else:\n",
    "        #print(f'{column} does not look like a normal distribution (reject H0)')\n",
    "        pass\n",
    "# Perform Shapiro-Wilk test to test for HRLYYEARN training data\n",
    "stat, p = stats.shapiro(data4['HRLYEARN'])\n",
    "if p > alpha:\n",
    "    #print(f'HRLYEARN looks like a normal distribution (fail to reject H0)')\n",
    "    pass\n",
    "else:\n",
    "    #print(f'HRLYEARN does not look like a normal distribution (reject H0)')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Linearity Assumptions Tested</span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No Log Transformation of the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data4['HRLYEARN']\n",
    "#The response variable\n",
    "\n",
    "# Fitting the model\n",
    "model = sm.OLS(y, features).fit()\n",
    "\n",
    "model_summary = model.summary()\n",
    "#print(model_summary)\n",
    "\n",
    "# Getting the residuals\n",
    "residuals = model.resid\n",
    "fitted_values=model.fittedvalues\n",
    "\n",
    "# Checking for independence (Durbin-Watson test)\n",
    "dw_test = sm.stats.stattools.durbin_watson(residuals)\n",
    "#print('Durbin-Watson test statistic:', dw_test)\n",
    "\n",
    "shapiro_test = stats.shapiro(residuals)\n",
    "#print('Shapiro-Wilk test statistic:', shapiro_test)\n",
    "\n",
    "mean_residuals = residuals.mean()\n",
    "#print('Mean of the residuals:', mean_residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Log Transformation of the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to the dependent variable to fix the normality issue and check the residuals again\n",
    "# This fixed the normaility issue but not the homoscedasticity issue\n",
    " \n",
    "#transformed dependent variable (y)\n",
    "log_y=np.log(y)\n",
    "\n",
    "# Fit the model\n",
    "log_model = sm.OLS(log_y, features).fit()\n",
    "\n",
    "# Get the residuals\n",
    "log_residuals = model.resid\n",
    "\n",
    "log_fitted_values = log_model.fittedvalues\n",
    "\n",
    "# Check for independence (Durbin-Watson test)\n",
    "dw_test = sm.stats.stattools.durbin_watson(residuals)\n",
    "#print('Durbin-Watson test statistic:', dw_test)\n",
    "\n",
    "shapiro_test = stats.shapiro(residuals)\n",
    "#print('Shapiro-Wilk test statistic:', shapiro_test)\n",
    "\n",
    "# Check for mean of residuals equal to zero\n",
    "mean_residuals = residuals.mean()\n",
    "#print('Mean of residuals:', mean_residuals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histograms of No Log, and Log Tranformations of the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_histogram([residuals,log_residuals],['Histogram of Residuals', 'Histogram of Log Residuals'],['Residuals','Residuals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatter Plots to Visualize the Common Variance of the Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatterplot(x,y, titles):\n",
    "    # Create subplots\n",
    "    fig, ax = plt.subplots(ncols=len(x), nrows=1, figsize=(20, 7))\n",
    "\n",
    "    # Plot each feature in a separate subplot\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        try:\n",
    "            axi.scatter(x=x[i],y=y[i], color='skyblue', edgecolor='black')\n",
    "            axi.set_title(titles[i], fontsize=14)\n",
    "            axi.set_xlabel(\"Fitted Values\", fontsize=12)\n",
    "            axi.set_ylabel('Residulas', fontsize=12)\n",
    "        except IndexError:\n",
    "            break\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#create_scatterplot([fitted_values,log_fitted_values], [residuals,log_residuals],['Fitted values vs Residuals','Log Fitted values vs Residuals'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Log Transform the Dependent Value</span>\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4['HRLYEARN']=np.log(data3['HRLYEARN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: #4682B4;\">Outlier Detection using IQR</span>\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature series to be plotted\n",
    "def create_box_plot(feature_series,feature_labels):\n",
    "    # Customize flier properties for outliers\n",
    "    flierprops = dict(marker='o', markerfacecolor='r', markersize=12,\n",
    "                    linestyle='none', markeredgecolor='g')\n",
    "    # Create subplots\n",
    "    fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(20, 7))\n",
    "\n",
    "    # Plot each feature in a separate subplot\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        try:\n",
    "            axi.boxplot(feature_series[i], flierprops=flierprops, patch_artist=True,\n",
    "                        boxprops=dict(facecolor='skyblue', color='black'),\n",
    "                        capprops=dict(color='black'),\n",
    "                        whiskerprops=dict(color='black'),\n",
    "                        medianprops=dict(color='red'))\n",
    "            axi.set_title(feature_labels[i], fontsize=14)\n",
    "            axi.tick_params(axis='y', labelsize=12)\n",
    "            axi.tick_params(axis='x', labelsize=0)\n",
    "        except IndexError:\n",
    "            break\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the box plots\n",
    "#create_box_plot([data4['HRLYEARN'], data4['UHRSMAIN'], data4['TENURE']],['Hourly Earnings (in dollars)', 'Usual Hours Worked (per week)', 'Job Tenure (in months)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating The 5 Number Summaries for the Quantitative Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 number summary for HRLYEARN.\n",
    "Q1_HRLYEARN = np.percentile(data3['HRLYEARN'], 25)\n",
    "Q2_HRLYEARN  = np.percentile(data3['HRLYEARN'], 50)\n",
    "Q3_HRLYEARN  = np.percentile(data3['HRLYEARN'], 75)\n",
    "min_value_HRLYEARN  = np.min(data3['HRLYEARN'])\n",
    "max_value_HRLYEARN  = np.max(data3['HRLYEARN'])\n",
    "IQR_HRLYEARN = Q3_HRLYEARN  - Q1_HRLYEARN #print(\"Five Number Summary Q1,Q2,Q3,min_value,max_value, and IQR:\",Q1_HRLYEARN ,Q2_HRLYEARN ,Q3_HRLYEARN , min_value_HRLYEARN ,max_value_HRLYEARN , IQR_HRLYEARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_UHRSMAIN = np.percentile(data3['UHRSMAIN'], 25)\n",
    "Q2_UHRSMAIN = np.percentile(data3['UHRSMAIN'], 50)\n",
    "Q3_UHRSMAIN = np.percentile(data3['UHRSMAIN'], 75)\n",
    "min_value_UHRSMAIN = np.min(data3['UHRSMAIN'])\n",
    "max_value_UHRSMAIN = np.max(data3['UHRSMAIN'])\n",
    "IQR_UHRSMAIN = Q3_UHRSMAIN - Q1_UHRSMAIN\n",
    "#print(\"Five Number Summary Q1,Q2,Q3,min_value,max_value, and IQR:\",Q1_UHRSMAIN,Q2_UHRSMAIN,Q3_UHRSMAIN, min_value_UHRSMAIN,max_value_UHRSMAIN, IQR_UHRSMAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_TENURE = np.percentile(data3['TENURE'], 25)\n",
    "Q2_TENURE = np.percentile(data3['TENURE'], 50)\n",
    "Q3_TENURE = np.percentile(data3['TENURE'], 75)\n",
    "min_value_TENURE = np.min(data3['TENURE'])\n",
    "max_value_TENURE = np.max(data3['TENURE'])\n",
    "IQR_TENURE = Q3_TENURE - Q1_TENURE\n",
    "#print(\"Five Number Summary Q1,Q2,Q3,min_value,max_value, and IQR:\",Q1_TENURE,Q2_TENURE,Q3_TENURE, min_value_TENURE,max_value_TENURE, IQR_TENURE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for outliers in the response variable using IQR\n",
    "outliers_HRLYEARN = data3[(data3['HRLYEARN'] < (Q1_HRLYEARN - 1.5 * IQR_HRLYEARN)) | (data3['HRLYEARN'] > (Q3_HRLYEARN + 1.5 * IQR_HRLYEARN))]\n",
    "outliers_UHRSMAIN = data3[(data3['UHRSMAIN'] < (Q1_UHRSMAIN - 1.5 * IQR_UHRSMAIN)) | (data3['UHRSMAIN'] > (Q3_UHRSMAIN + 1.5 * IQR_UHRSMAIN))]\n",
    "outliers_TENURE = data3[(data3['TENURE'] < (Q1_TENURE - 1.5 * IQR_TENURE)) | (data3['TENURE'] > (Q3_TENURE + 1.5 * IQR_TENURE))]\n",
    "#print(\"The number of outliers for HRLYEARN, UHRSMAIN, and TENURE are:\",len(outliers_HRLYEARN),len(outliers_UHRSMAIN),len(outliers_TENURE))\n",
    "#print(\"The shape of the working dataset is:\",data3.shape)\n",
    "#The outliers account for approximately 18% of the training data. The outliers will be removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #8FBC8F;\">Saving Checkpoint 3</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5=data4.copy()\n",
    "#data5.info()\n",
    "# Creating new data frame after removing the outliers\n",
    "# Crating a newdataframe to remove the outliers from data3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #BDB76B;\"> Removing Outliers from the Quantitative Variables</span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the outliers from the dataset\n",
    "data5 = data5[(data5['HRLYEARN'] >= (Q1_HRLYEARN - 1 * IQR_HRLYEARN)) & (data5['HRLYEARN'] <= (Q3_HRLYEARN + 1.5 * IQR_HRLYEARN))]\n",
    "data5 = data5[(data5['UHRSMAIN'] >= (Q1_UHRSMAIN - 1 * IQR_UHRSMAIN)) & (data5['UHRSMAIN'] <= (Q3_UHRSMAIN + 1.5 * IQR_UHRSMAIN))]\n",
    "data5 = data5[(data5['TENURE'] >= (Q1_TENURE - 1 * IQR_TENURE)) & (data5['TENURE'] <= (Q3_TENURE + 1.5 * IQR_TENURE))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics_table_continuous_variables_before_outliers=statistics_table(data4[['HRLYEARN','UHRSMAIN','TENURE']])\n",
    "summary_statistics_table_continuous_variables_after_outliers=statistics_table(data5[['HRLYEARN','UHRSMAIN','TENURE']])\n",
    "# Creating the summary statistics table for the continuous variables before and after removing the outliers\n",
    "#print(summary_statistics_table_continuous_variables_before_outliers)\n",
    "#print(summary_statistics_table_continuous_variables_after_outliers)\n",
    "\n",
    "#summary_statistics_table_continuous_variables_before_outliers.to_excel('Continuous Variables Before Removing outliers.xlsx',index=False)  \n",
    "#summary_statistics_table_continuous_variables_after_outliers.to_excel('Continuous Variables After Removing outliers.xlsx',index=False)  \n",
    "# Exporting the summary statistics table for the continuous variables before and after removing the outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #2F4F4F;\">Apriori Algorithm</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <span style=\"color: #8FBC8F;\">Copying the data from Checkpoint 3</span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_data= data5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\">  Apriori Pre-Processing</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>  Creatings Bins for the Aprirori Algorithm</span>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_creator(numerical_variables):\n",
    "\n",
    "    if isinstance(numerical_variables, pd.Series):\n",
    "        variables=numerical_variables.to_frame()\n",
    "        bin_maker = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "        variables=bin_maker.fit_transform(variables)\n",
    "        # Getting the bin intervals\n",
    "        bin_edges = bin_maker.bin_edges_\n",
    "        # Creating a dictionary to map feature names to the respective bin intervals\n",
    "        bin_edges_dict = {feature: edges for feature, edges in zip(numerical_variables, bin_edges)}\n",
    "        # Printing bin interval for each feature\n",
    "        for feature, edges in bin_edges_dict.items():\n",
    "            #print(f\"Bin interval for {feature}: {edges}\")\n",
    "            pass\n",
    "        bin_info = []\n",
    "        for feature, edges in bin_edges_dict.items():\n",
    "            for i in range(len(edges) - 1):\n",
    "                bin_info.append({\n",
    "                    'Feature': feature,\n",
    "                    'Bin Index': i,\n",
    "                    'Bin Start': edges[i],\n",
    "                    'Bin End': edges[i + 1]\n",
    "                })\n",
    "        # Creating a DataFrame \n",
    "        bin_info_df = pd.DataFrame(bin_info)\n",
    "        # Displaying the DataFrame\n",
    "        print(bin_info_df)\n",
    "        #bin_info_df.to_excel('Binning Information Table.xlsx',index=False)\n",
    "        return variables\n",
    "\n",
    "    else:\n",
    "        columns=list(numerical_variables.columns)\n",
    "        variables=numerical_variables[columns]\n",
    "        # Initializing the KBinsDiscretizer with 'quantile' strategy so that the labels are balanced. \n",
    "        bin_maker = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "        # Fitting and transforming \n",
    "        variables[columns]= bin_maker.fit_transform(variables)\n",
    "        # Getting the bin intervals\n",
    "        bin_edges = bin_maker.bin_edges_\n",
    "        # Creating a dictionary to map feature names to the respective bin intervals\n",
    "        bin_edges_dict = {feature: edges for feature, edges in zip(numerical_variables, bin_edges)}\n",
    "        # Printing bin interval for each feature\n",
    "        for feature, edges in bin_edges_dict.items():\n",
    "            #print(f\"Bin interval for {feature}: {edges}\")\n",
    "            pass\n",
    "        bin_info = []\n",
    "        for feature, edges in bin_edges_dict.items():\n",
    "            for i in range(len(edges) - 1):\n",
    "                bin_info.append({\n",
    "                    'Feature': feature,\n",
    "                    'Bin Index': i,\n",
    "                    'Bin Start': edges[i],\n",
    "                    'Bin End': edges[i + 1]\n",
    "                })\n",
    "        # Creating a DataFrame \n",
    "        bin_info_df = pd.DataFrame(bin_info)\n",
    "        # Display the DataFrame\n",
    "        #print(bin_info_df)\n",
    "        #bin_info_df.to_excel('Binning Information Table.xlsx',index=False)\n",
    "        return variables[columns]\n",
    "\n",
    "training_bins=bin_creator(categorical_data[['UHRSMAIN', 'TENURE','HRLYEARN']])\n",
    "testing_bins=bin_creator(categorical_data[['UHRSMAIN', 'TENURE','HRLYEARN']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span >  Creating the Dummy Variables for the Apriori Alogirthm</span>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_creator(dummy, dummy2):\n",
    "    columns = list(dummy.columns)\n",
    "    dummy_creator = OneHotEncoder(sparse=False)\n",
    "    encoded_features = dummy_creator.fit_transform(dummy)\n",
    "    feature_names = dummy_creator.get_feature_names_out(columns)\n",
    "    categorical_data = pd.DataFrame(encoded_features, columns=feature_names, index=dummy.index)\n",
    "    \n",
    "    remaining_columns = dummy2.drop(columns=columns)\n",
    "    other_columns = list(remaining_columns.columns)\n",
    "    encoded_other_features = dummy_creator.fit_transform(remaining_columns)\n",
    "    other_feature_names = dummy_creator.get_feature_names_out(other_columns)\n",
    "    other_categorical_data = pd.DataFrame(encoded_other_features, columns=other_feature_names, index=remaining_columns.index)\n",
    "    \n",
    "    combined_data = categorical_data.join(other_categorical_data, how='inner')\n",
    "    return combined_data\n",
    "\n",
    "categorical_data = dummy_creator(training_bins, categorical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\">  Replication</span>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the Apriori algorithm\n",
    "frq_itemset = apriori(categorical_data, min_support=0.05, use_colnames=True)\n",
    "#print(\"Frequent Itemsets:\\n\", frq_itemset.head())\n",
    "# Support set ato 5% because 25% was too high\n",
    "\n",
    "# Association Rules\n",
    "rules = association_rules(frq_itemset, metric=\"confidence\", min_threshold=0.01)\n",
    "#print(\"Association rules:\\n\", rules.head())\n",
    "#Confidence set 1% because 80% was too high\n",
    "\n",
    "# Defining Right Hand Side\n",
    "rhs = ['HRLYEARN_3.0', 'HRLYEARN_4.0']\n",
    "\n",
    "# Filtering the rules by RHS\n",
    "rules = rules[rules['consequents'].apply(lambda x: any(item in x for item in rhs))]\n",
    "\n",
    "print(\"Association Rules:\\n\", rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a report with the important rules\n",
    "report = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
    "\n",
    "#report.to_excel('rules_hrlyearn.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\">  Change</span>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #2F4F4F;\">Non-Linear Regression</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\">  Creating the Features and Lables For Non-Linear Regeression</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=data5.copy()\n",
    "features=features.drop(columns=['HRLYEARN'])\n",
    "\n",
    "#dropping these features because they are scaled, and we need to keep the original values for the non-linear models, and scaled later.\n",
    "continuous_features=data5[['TENURE', 'UHRSMAIN']]\n",
    "\n",
    "label = data5['HRLYEARN']\n",
    "#These will be used for the non-linear machine learning algoirthms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\"> Non-Linear Regression Replication</span>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-Linear Train, Test, Split for Base Models without Hyper Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>  Non-Linear Regression Pre-processing for Base Models without Hyper Tuning</span>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), list(continuous_features.columns)),\n",
    "        ('cat', OneHotEncoder(drop='first'), list(features.drop(columns=['TENURE', 'UHRSMAIN']).columns))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Linear Base Models without Hyper Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model=SVR(kernel='rbf')\n",
    "rf_model=RandomForestRegressor(n_estimators=100,random_state=42)\n",
    "xg_model=XGBRegressor(seed=42)\n",
    "rt_model=DecisionTreeRegressor(random_state=42)\n",
    "knn_model=KNeighborsRegressor()\n",
    "\n",
    "models=[rf_model,svr_model,xg_model,knn_model,rt_model]\n",
    "#models=[knn_model]\n",
    "\n",
    "train_score=[]\n",
    "test_score=[]\n",
    "for model in models:\n",
    "    pipeline=Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    train_score.append(mean_squared_error(y_test, predictions, squared=False))\n",
    "\n",
    "    print(f\"RMSE for {model}:\", mean_squared_error(y_test, predictions, squared=False))\n",
    "    \n",
    "print(pd.DataFrame({'Train Score':list(np.array(train_score).reshape(-1,1))}, index=models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <span>  Kruskal-Wallis Test for Non-Linear Regression</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nlr_results = pd.DataFrame({ \n",
    "    'Model': ['Random Forest', 'Support Vector', 'XGBoost', 'KNN', 'Regression Tree'],\n",
    "    'RMSE': [0.26, 0.31, 0.31, 0.33, 0.35]\n",
    "})\n",
    "\n",
    "# Perform the Kruskal-Wallis test\n",
    "statistic, p_value = kruskal(\n",
    "    data_nlr_results[data_nlr_results['Model'] == 'Random Forest']['RMSE'],\n",
    "    data_nlr_results[data_nlr_results['Model'] == 'Support Vector']['RMSE'],\n",
    "    data_nlr_results[data_nlr_results['Model'] == 'XGBoost']['RMSE'],\n",
    "    data_nlr_results[data_nlr_results['Model'] == 'KNN']['RMSE'],\n",
    "    data_nlr_results[data_nlr_results['Model'] == 'Regression Tree']['RMSE']\n",
    ")\n",
    "\n",
    "print(f\"Kruskal-Wallis test statistic: {statistic}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\"> Non-Linear Regression Hyper Tuned</span>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-Linear Regression Hyper Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is Non-Linear Regression hyper tuning\n",
    "continuous_features = ['UHRSMAIN', 'TENURE']\n",
    "categorical_features = ['PROV', 'CMA', 'AGE', 'SEX', 'MARSTAT', 'EDUC', 'IMMIG', 'NAICS', 'UNION', 'FIRMSIZE']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), continuous_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])\n",
    "#Setting up the column transformer\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#Initializing K-Folds\n",
    "\n",
    "param_grids = {\n",
    "    'XGB': {\n",
    "        'model__n_estimators': np.arange(100, 1100, 1),\n",
    "        'model__learning_rate': np.logspace(-3, 0, 100),\n",
    "        'model__max_depth': np.arange(1, 11, 1),\n",
    "        'model__subsample': np.arange(0.5, 1.0, 0.1),\n",
    "        'model__colsample_bytree': np.arange(0.5, 1.0, 0.1),\n",
    "        'model__gamma': np.arange(0, 5, 1),\n",
    "        'model__reg_alpha': np.logspace(-3, 2, 100),\n",
    "    },\n",
    "    'DT': {\n",
    "        'model__criterion': ['mse', 'mae'],\n",
    "        'model__splitter': ['best', 'random'],\n",
    "        'model__max_depth': np.arange(1, 11, 1),\n",
    "        'model__min_samples_split': np.arange(2, 10, 1),\n",
    "        'model__min_samples_leaf': np.arange(1, 10, 1)\n",
    "    },\n",
    "    'RF': {\n",
    "        'model__n_estimators': np.arange(100, 1100, 1),\n",
    "        'model__max_features': ['sqrt', 'log2'],\n",
    "        'model__max_depth': np.arange(1, 110, 1),\n",
    "        'model__min_samples_split': np.arange(2, 10, 1),\n",
    "        'model__min_samples_leaf': np.arange(1, 10, 1)\n",
    "\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'model__gamma': ['scale', 'auto'],\n",
    "        'model__C': np.logspace(-3,2,100)\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model__n_neighbors': np.arange(1, 20, 1),\n",
    "        'model__weights': ['uniform', 'distance'],\n",
    "        'model__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'model__p': np.arange(1, 5, 1)\n",
    "\n",
    "    }\n",
    "}\n",
    "#Defining the parameter grids for each model\n",
    "\n",
    "\n",
    "models = {\n",
    "    'XGB': XGBRegressor(seed=42)\n",
    "    ,'DT': DecisionTreeRegressor(random_state=42)\n",
    "    ,'RF': RandomForestRegressor(random_state=42)\n",
    "    ,'SVR': SVR(random_state=42)\n",
    "    ,'KNN': KNeighborsRegressor()\n",
    "}\n",
    "# Defining each model\n",
    "\n",
    "# Perform RandomizedSearchCV for each model\n",
    "scores = []\n",
    "#creating an empty list to store the scores\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    grid_search = RandomizedSearchCV(pipeline, param_distributions=param_grids[model_name], n_iter=1, cv=kf, random_state=42, scoring='neg_root_mean_squared_error',n_jobs=-1)\n",
    "    grid_result = grid_search.fit(features, label)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_result.best_score_,\n",
    "        'best_params': grid_result.best_params_\n",
    "    })\n",
    "#Running the randomized search for each model\n",
    "\n",
    "scores_df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "# Creating a datafreame for all the scores\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(0.2571711415225991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #2F4F4F;\">Classification</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\"> Buliding the Dataset for Classifiers</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dependent variable into 5 categorical values\n",
    "#features was created for non-linear regression\n",
    "#continuous_features list that contains usual hours worked and tenure was created for non-linear regression\n",
    "\n",
    "#print(label.head())\n",
    "classified_labels=bin_creator(label)\n",
    "#print(\"Label Shape: \", classified_labels.shape)\n",
    "#print(\"Features Shape: \",features.shape\n",
    "classified_labels.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\"> Classifcation Replication</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>  Classification Train, Test, Split for Base Models without Hyper Tuning</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, classified_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>  Classification Pre-processing for Base Models without Hyper Tuning</span>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), list(continuous_features.columns)),\n",
    "        ('cat', OneHotEncoder(drop='first'), list(features.drop(columns=['TENURE', 'UHRSMAIN']).columns))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span>  Classification Base Models</span>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model=SVC(kernel='rbf',random_state=42)\n",
    "rf_model=RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "nb_model = MultinomialNB()\n",
    "lr_model=LogisticRegression(random_state=42)\n",
    "xg_model=XGBClassifier(seed=42)\n",
    "rt_model=DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "models=[rf_model,svc_model,nb_model,xg_model,lr_model,rt_model]\n",
    "\n",
    "train_score=[]\n",
    "test_score=[]\n",
    "for model in models:\n",
    "    pipeline=Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    train_score.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "    print(f\"Accuracy Score for {model}:\", accuracy_score(y_test, predictions))\n",
    "    print(f\"Percision Score for {model}:\", precision_score(y_test, predictions,average='weighted'))\n",
    "    print(f\"Recall Score for {model}:\", recall_score(y_test, predictions,average='weighted'))\n",
    "    print(f\"F1-Score for {model}:\", f1_score(y_test, predictions,average='weighted'))\n",
    "    report = classification_report(y_test, predictions, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    #print(display(report_df))\n",
    "\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "print(pd.DataFrame({'Train Score':list(np.array(train_score).reshape(-1,1))}, index=models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### <span>  Friedman Test for Classification</span>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classification_results = pd.DataFrame({ \n",
    "    'Model': [\n",
    "        'Random Forest', \n",
    "        'Decision Tree', \n",
    "        'XGBoost', \n",
    "        'SVC', \n",
    "        'Logistic Regression', \n",
    "        'MultinomialNB'\n",
    "    ],\n",
    "    'Accuracy': [70.3, 63.2, 50.2, 49.0, 42.6, 39.9],\n",
    "    'Recall': [70.3, 63.2, 50.2, 49.0, 42.6, 39.9],\n",
    "    'Precision': [70.2, 63.2, 49.4, 48.3, 41.3, 38.1],\n",
    "    'F1-Score': [70.2, 63.2, 49.6, 48.5, 41.6, 38.4]\n",
    "})\n",
    "\n",
    "accuracy = data_classification_results['Accuracy'].values\n",
    "recall = data_classification_results['Recall'].values\n",
    "precision = data_classification_results['Precision'].values\n",
    "f1_score = data_classification_results['F1-Score'].values\n",
    "# Friedman test\n",
    "statistic, p_value = friedmanchisquare(accuracy, recall, precision, f1_score)\n",
    "\n",
    "print(f\"Friedman T-Statistic: {statistic}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #4682B4;\">  Classification Hyper Tuned</span>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span >  Classification Models Hyper Tuned</span>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor=ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), list(continuous_features.columns)),\n",
    "        ('cat', OneHotEncoder(drop='first'), list(features.drop(columns=['TENURE', 'UHRSMAIN']).columns))\n",
    "    ])\n",
    "#Setting up the column transformer\n",
    "\n",
    "kf = KFold(n_splits=6, shuffle=True, random_state=42)\n",
    "#Initializing K-Folds\n",
    "\n",
    "param_grids = {\n",
    "    'svm': {\n",
    "        'model__C': [0.1, 1, 10, 100],\n",
    "        'model__kernel': ['rbf'],\n",
    "        'model__degree': [2, 3, 4, 5],\n",
    "        'model__gamma': ['scale', 'auto']\n",
    "        \n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model__max_depth':[1,10,20,30,40,50,60,70,80,90,100],\n",
    "        'model__min_samples_split':[2,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100]\n",
    "        \n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model__n_estimators': [1,5,10,50,100,200,400,600,1200,2400]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "            'model__solver':['newton-cg'],\n",
    "            'model__C':np.logspace(-3,2,100),\n",
    "            'model__penalty' : ['l2']\n",
    "    }\n",
    "    ,\n",
    "    'naive_bayes': {\n",
    "        'model__alpha': [0.1, 0.5, 1, 2, 5]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model__n_estimators': [1,5,10,50,100,200,400,600,1200,2400],\n",
    "        'model__learning_rate': [0.1, 0.01, 0.001],\n",
    "        'model__max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'model__subsample': [0.7, 0.8, 0.9],\n",
    "        'model__colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'model__gamma': [0, 1, 5],\n",
    "        'model__reg_alpha': [0, 1, 5]\n",
    "    }\n",
    "}\n",
    "#Defining the parameter grids for each model\n",
    "\n",
    "\n",
    "models = {\n",
    "    'svm': SVC(random_state=42)\n",
    "    ,'decision_tree': DecisionTreeRegressor(random_state=42)\n",
    "    ,'naive_bayes': MultinomialNB()\n",
    "    ,'xgboost': XGBClassifier(seed=42)\n",
    "    ,'random_forest': RandomForestRegressor(random_state=42)\n",
    "    ,'logistic_regression': LogisticRegression(random_state=42)\n",
    "\n",
    "}\n",
    "# Defining each model\n",
    "\n",
    "# Perform RandomizedSearchCV for each model\n",
    "scores = []\n",
    "#creating an empty list to store the scores\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipeline_classifier = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    grid_search = RandomizedSearchCV(pipeline_classifier, param_distributions=param_grids[model_name], n_iter=5, cv=kf, random_state=42, n_jobs=-1)\n",
    "    grid_result = grid_search.fit(features, classified_labels)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_result.best_score_,\n",
    "        'best_params': grid_result.best_params_\n",
    "    })\n",
    "#Running the randomized search for each model\n",
    "\n",
    "scores_df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "# Creating a datafreame for all the scores\n",
    "print(scores_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
