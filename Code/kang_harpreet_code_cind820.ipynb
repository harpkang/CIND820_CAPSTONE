{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Capstone Project: CIND820 \n",
    "Harpreet Kang\n",
    "June 7, 2024\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ydata_profiling import ProfileReport\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV,KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Displaying all the columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('dataset_lfs_2024.xlsx')\n",
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[\"HRLYEARN\"],inplace=True)\n",
    "# Removing the null values in HRLYEARN column\n",
    "data=data[data['MJH']==1]\n",
    "# Filtering for single job holders\n",
    "data=data[data['SCHOOLN']==1]\n",
    "# Filtering for non students\n",
    "data=data[data['FTPTMAIN']==1]\n",
    "# Filtering for full-time workers\n",
    "data=data[data['PERMTEMP']==1]\n",
    "# Filtering for permanent workers\n",
    "data['HRLYEARN']=data['HRLYEARN']/100\n",
    "# The data dictionary for this dataset indicated that the last 2 values of this numeric column were the decimal points. So dividing the HRLYEARN column by 100 will add 2 decimal points. \n",
    "data['UHRSMAIN']=data['UHRSMAIN']/10\n",
    "data['AHRSMAIN']=data['AHRSMAIN']/10\n",
    "data['UTOTHRS']=data['UTOTHRS']/10\n",
    "data['ATOTHRS']=data['ATOTHRS']/10\n",
    "data['HRSAWAY']=data['HRSAWAY']/10\n",
    "data['PAIDOT']=data['PAIDOT']/10\n",
    "data['UNPAIDOT']=data['UNPAIDOT']/10\n",
    "data['XTRAHRS']=data['XTRAHRS']/10\n",
    "# # The data dictionary for this dataset indicated that the last 1 values of the 8 numeric columns above were the decimal points. So dividing the UHRSMAIN column by 100 will add 1 decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile1 = ProfileReport(data, title=\"Profiling Report\")\n",
    "#profile1.to_file(\"EDA after eliminating blanks of Usual Hourly Wages (HRLYEARN).html\")\n",
    "# Generating the profiling report after removing the null values in HRLYEARN column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caclulating the high imbalances for variables: LFSSTAT ,MJH, FTPTMAIN, PERMTEMP, SCHOOLN ,HRSAWAY, PAIDOT, UNPAIDOT, XTRAHRS\n",
    "\n",
    "#for column in ['LFSSTAT','MJH','FTPTMAIN','PERMTEMP','SCHOOLN','HRSAWAY','PAIDOT','UNPAIDOT','XTRAHRS']:\n",
    "    #print(data[column].value_counts(normalize=True)*100)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_variable_name_mapping={\n",
    "'rec_num':'Order of record in file',\n",
    "'survyear':'Survey year',\n",
    "'survmnth':'Survey month',\n",
    "'lfsstat':'Labour force status',\n",
    "'prov':'Province',\n",
    "'cma':'Nine largest CMAs',\n",
    "'age_12':'Five-year age group of respondent',\n",
    "'age_6':'Age in 2 and 3 year groups, 15 to 29',\n",
    "'sex':'Sex of respondent',\n",
    "'marstat':'Marital status of respondent',\n",
    "'educ':'Highest educational attainment',\n",
    "'mjh':'Single or multiple jobholder',\n",
    "'everwork':'Identifies if a person has worked in the last year',\n",
    "'ftptlast':'Full- or part-time status of last job',\n",
    "'cowmain':'Class of worker, main job',\n",
    "'immig':'Immigrant status',\n",
    "'naics_21':'Industry of main job',\n",
    "'noc_10':'Occupation at main job (noc_10)',\n",
    "'noc_43':'Occupation at main job (noc_43)',\n",
    "'yabsent':'Reason of absence, full week',\n",
    "'wksaway':'Number of weeks absent from work',\n",
    "'payaway':'Paid for time off, full-week absence only',\n",
    "'uhrsmain':'Usual hours worked per week at main job',\n",
    "'ahrsmain':'Actual hours worked per week at main job',\n",
    "'ftptmain':'Full- or part-time status at main or only job',\n",
    "'utothrs':'Usual hours worked per week at all jobs',\n",
    "'atothrs':'Actual hours worked per week at all jobs',\n",
    "'hrsaway':'Hours away from work, part-week absence only',\n",
    "'yaway':'Reason for part-week absence',\n",
    "'paidot':'Paid overtime hours in reference week',\n",
    "'unpaidot':'Unpaid overtime hours in reference week',\n",
    "'xtrahrs':'Number of overtime or extra hours worked',\n",
    "'whypt':'Reason for part-time work',\n",
    "'tenure':'Job tenure with current employer',\n",
    "'prevten':'Job tenure with previous employer',\n",
    "'hrlyearn':'Usual hourly wages',\n",
    "'union':'Union status',\n",
    "'permtemp':'Job permanency',\n",
    "'estsize':'Establishment size',\n",
    "'firmsize':'Firm size',\n",
    "'durunemp':'Duration of unemployment',\n",
    "'flowunem':'Flows into unemployment',\n",
    "'unemftpt':'Job seekers by type of work sought and temporary layoffs by work status of last job',\n",
    "'whylefto':'Reason for leaving job during previous year (whylefto)',\n",
    "'whyleftn':'Reason for leaving job during previous year (whyleftn)',\n",
    "'durjless':'Duration of joblessness',\n",
    "'availabl':'Availability during the reference week',\n",
    "'lkpubag':'Unemployed, used public employment agency',\n",
    "'lkemploy':'Unemployed, checked with employers directly',\n",
    "'lkrels':'Unemployed, checked with friends or relatives',\n",
    "'lkatads':'Unemployed, looked at job ads',\n",
    "'lkansads':'Unemployed, placed or answered ads',\n",
    "'lkothern':'Unemployed, other methods',\n",
    "'prioract':'Main activity before started looking for work',\n",
    "'ynolook':'Reason for not looking for work during the reference week',\n",
    "'tlolook':'Temporary layoff, looked for work during the last four weeks',\n",
    "'schooln':'Current student status',\n",
    "'efamtype':'Type of economic family',\n",
    "'agyownk':'Age of youngest child',\n",
    "'finalwt':'Standard final weight'}\n",
    "\n",
    "# Mapping descriptions to the variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.describe()\n",
    "# Displaying the summary statistics of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.info()\n",
    "# Displaying the data types of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_missing_values= data.isnull().sum()\n",
    "# Counting the missing values in each column\n",
    "data_percent_missing = round(data.isnull().sum() * 100 / len(data),2)\n",
    "# Calculating the percentage of missing values in each column\n",
    "missing_values_percent=pd.DataFrame(data_percent_missing, columns=['Missing Values %'])\n",
    "# Creating a dataframe to display the missing values percentage\n",
    "missing_values_percent=missing_values_percent.rename_axis('Variables')\n",
    "# Renaming the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()\n",
    "# Displaying the first 5 rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_table(data_frame):\n",
    "    data_type_mapping={\n",
    "    'rec_num':'Nominal',\n",
    "    'survyear':'Ordinal',\n",
    "    'survmnth':'Ordinal',\n",
    "    'lfsstat':'Nominal',\n",
    "    'prov':'Nominal',\n",
    "    'cma':'Nominal',\n",
    "    'age_12':'Ordinal',\n",
    "    'age_6':'Ordinal',\n",
    "    'sex':'Nominal',\n",
    "    'marstat':'Nominal',\n",
    "    'educ':'Ordinal',\n",
    "    'mjh':'Nominal',\n",
    "    'everwork':'Nominal',\n",
    "    'ftptlast':'Nominal',\n",
    "    'cowmain':'Nominal',\n",
    "    'immig':'Nominal',\n",
    "    'naics_21':'Nominal',\n",
    "    'noc_10':'Nominal',\n",
    "    'noc_43':'Nominal',\n",
    "    'yabsent':'Nominal',\n",
    "    'wksaway':'Nominal',\n",
    "    'payaway':'Nominal',\n",
    "    'uhrsmain':'Continuous',\n",
    "    'ahrsmain':'Continuous',\n",
    "    'ftptmain':'Nominal',\n",
    "    'utothrs':'Continuous',\n",
    "    'atothrs':'Continuous',\n",
    "    'hrsaway':'Continuous',\n",
    "    'yaway':'Nominal',\n",
    "    'paidot':'Continuous',\n",
    "    'unpaidot':'Continuous',\n",
    "    'xtrahrs':'Continuous',\n",
    "    'whypt':'Nominal',\n",
    "    'tenure':'Discrete',\n",
    "    'prevten':'Discrete',\n",
    "    'hrlyearn':'Continuous',\n",
    "    'union':'Nominal',\n",
    "    'permtemp':'Nominal',\n",
    "    'estsize':'Ordinal',\n",
    "    'firmsize':'Ordinal',\n",
    "    'durunemp':'Discrete',\n",
    "    'flowunem':'Nominal',\n",
    "    'unemftpt':'Nominal',\n",
    "    'whylefto':'Nominal',\n",
    "    'whyleftn':'Nominal',\n",
    "    'durjless':'Discrete',\n",
    "    'availabl':'Nominal',\n",
    "    'lkpubag':'Nominal',\n",
    "    'lkemploy':'Nominal',\n",
    "    'lkrels':'Nominal',\n",
    "    'lkatads':'Nominal',\n",
    "    'lkansads':'Nominal',\n",
    "    'lkothern':'Nominal',\n",
    "    'prioract':'Nominal',\n",
    "    'ynolook':'Nominal',\n",
    "    'tlolook':'Nominal',\n",
    "    'schooln':'Nominal',\n",
    "    'efamtype':'Nominal',\n",
    "    'agyownk':'Ordinal',\n",
    "    'finalwt':'Continuous'}\n",
    "\n",
    "    data_object_mapping={\n",
    "    'rec_num':'Qualitative',\n",
    "    'survyear':'Qualitative',\n",
    "    'survmnth':'Qualitative',\n",
    "    'lfsstat':'Qualitative',\n",
    "    'prov':'Qualitative',\n",
    "    'cma':'Qualitative',\n",
    "    'age_12':'Qualitative',\n",
    "    'age_6':'Qualitative',\n",
    "    'sex':'Qualitative',\n",
    "    'marstat':'Qualitative',\n",
    "    'educ':'Qualitative',\n",
    "    'mjh':'Qualitative',\n",
    "    'everwork':'Qualitative',\n",
    "    'ftptlast':'Qualitative',\n",
    "    'cowmain':'Qualitative',\n",
    "    'immig':'Qualitative',\n",
    "    'naics_21':'Qualitative',\n",
    "    'noc_10':'Qualitative',\n",
    "    'noc_43':'Qualitative',\n",
    "    'yabsent':'Qualitative',\n",
    "    'wksaway':'Qualitative',\n",
    "    'payaway':'Qualitative',\n",
    "    'uhrsmain':'Quantitative',\n",
    "    'ahrsmain':'Quantitative',\n",
    "    'ftptmain':'Qualitative',\n",
    "    'utothrs':'Quantitative',\n",
    "    'atothrs':'Quantitative',\n",
    "    'hrsaway':'Quantitative',\n",
    "    'yaway':'Qualitative',\n",
    "    'paidot':'Quantitative',\n",
    "    'unpaidot':'Quantitative',\n",
    "    'xtrahrs':'Quantitative',\n",
    "    'whypt':'Qualitative',\n",
    "    'tenure':'Quantitative',\n",
    "    'prevten':'Quantitative',\n",
    "    'hrlyearn':'Quantitative',\n",
    "    'union':'Qualitative',\n",
    "    'permtemp':'Qualitative',\n",
    "    'estsize':'Qualitative',\n",
    "    'firmsize':'Qualitative',\n",
    "    'durunemp':'Qualitative',\n",
    "    'flowunem':'Qualitative',\n",
    "    'unemftpt':'Qualitative',\n",
    "    'whylefto':'Qualitative',\n",
    "    'whyleftn':'Qualitative',\n",
    "    'durjless':'Qualitative',\n",
    "    'availabl':'Qualitative',\n",
    "    'lkpubag':'Qualitative',\n",
    "    'lkemploy':'Qualitative',\n",
    "    'lkrels':'Qualitative',\n",
    "    'lkatads':'Qualitative',\n",
    "    'lkansads':'Qualitative',\n",
    "    'lkothern':'Qualitative',\n",
    "    'prioract':'Qualitative',\n",
    "    'ynolook':'Qualitative',\n",
    "    'tlolook':'Qualitative',\n",
    "    'schooln':'Qualitative',\n",
    "    'efamtype':'Qualitative',\n",
    "    'agyownk':'Qualitative',\n",
    "    'finalwt':'Quantitative'}\n",
    "    data_table=data_frame.columns.to_frame(index=False)\n",
    "    data_table=data_table.rename(columns={0:'Variable Name'})\n",
    "    columns=list(data_table.iloc[:,0] )\n",
    "    data_table_mean_lst=[]\n",
    "    data_table_median_lst=[]\n",
    "    data_table_min_lst=[]\n",
    "    data_table_max_lst=[]\n",
    "    data_table_std_lst=[]\n",
    "    data_table_object_lst=[]\n",
    "    data_table_dtype_lst=[]\n",
    "\n",
    "    for column in columns:\n",
    "        data_table_mean_lst.append(round(data_frame[column].mean(),2))\n",
    "        data_table_median_lst.append(round(data_frame[column].median(),2))\n",
    "        data_table_min_lst.append(round(data_frame[column].min(),2))\n",
    "        data_table_max_lst.append(round(data_frame[column].max(),2))\n",
    "        data_table_std_lst.append(round(data_frame[column].std(),2))\n",
    "        data_table_object_lst.append(data_object_mapping.get((column.lower())))\n",
    "        data_table_dtype_lst.append(data_type_mapping.get((column.lower())))\n",
    "    summary_statistics_table=pd.DataFrame(list(zip(columns,data_table_object_lst,data_table_dtype_lst,data_table_mean_lst,data_table_median_lst,data_table_min_lst,data_table_max_lst,data_table_std_lst)),\n",
    "                                      columns=['Variable','Data','Data Type','Mean','Median','Min','Max','Standard Deviation'])\n",
    "    return summary_statistics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics_table=statistics_table(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0f/v6wgxn554tx62wm1nqjqw5hw0000gn/T/ipykernel_3444/1696420439.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  summary_statistics_table_1[\"Description\"]=summary_statistics_table_1[\"Variable\"].apply(lambda x: full_variable_name_mapping.get(x.lower()))\n"
     ]
    }
   ],
   "source": [
    "#removing means from non continous and discrete variables\n",
    "summary_statistics_table[\"Mean\"] = summary_statistics_table.apply(lambda row: row[\"Mean\"] if row[\"Data Type\"].lower() in [\"continuous\", \"discrete\"] else None, axis=1)\n",
    "summary_statistics_table[\"Mean\"] = summary_statistics_table.apply(lambda row: row[\"Mean\"] if row[\"Data Type\"].lower() in [\"continuous\", \"discrete\"] else None, axis=1)\n",
    "summary_statistics_table[\"Median\"] = summary_statistics_table.apply(lambda row: None if row[\"Data Type\"].lower() == \"nominal\" else row[\"Median\"] , axis=1)\n",
    "summary_statistics_table=summary_statistics_table.merge(missing_values_percent,left_on='Variable',right_index=True)\n",
    "summary_statistics_table.sort_values(by=['Missing Values %','Variable'],inplace=True)\n",
    "summary_statistics_table.reset_index(drop=True,inplace=True)\n",
    "\n",
    "summary_statistics_table_1=summary_statistics_table[['Variable','Data','Data Type']]\n",
    "summary_statistics_table_1[\"Description\"]=summary_statistics_table_1[\"Variable\"].apply(lambda x: full_variable_name_mapping.get(x.lower()))\n",
    "summary_statistics_table_1=summary_statistics_table_1[['Variable','Description','Data','Data Type']]\n",
    "#summary_statistics_table_1.to_excel('Summary Statistics Table_Figure_3.xlsx',index=False)  \n",
    "\n",
    "summary_statistics_table_2=summary_statistics_table.drop(columns=['Data','Data Type'],axis=1)\n",
    "#summary_statistics_table_2.to_excel('Summary Statistics Table_Figure_2.xlsx',index=False)  \n",
    "\n",
    "# Creating a function to generate the statistics table for excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_attributes=list(summary_statistics_table[summary_statistics_table_2['Missing Values %']>56.00]['Variable'])\n",
    "#creating a list of variables with more than 56% missing values\n",
    "\n",
    "# Dropping the below variables as they are not important\n",
    "remove_attributes.append('SURVMNTH')\n",
    "remove_attributes.append('SURVYEAR')\n",
    "remove_attributes.append('REC_NUM')\n",
    "remove_attributes.append('FINALWT')\n",
    "\n",
    "##################################\n",
    "\n",
    "remove_attributes.append('LFSSTAT')\n",
    "remove_attributes.append('MJH')\n",
    "remove_attributes.append('FTPTMAIN')\n",
    "#deleting because we are only looking at full timer workers not parttime.\n",
    "remove_attributes.append('PERMTEMP')\n",
    "#already filtered for full time jobs\n",
    "remove_attributes.append('SCHOOLN')\n",
    "#only looking at non students and not full time or part time students\n",
    "\n",
    "##################################\n",
    " \n",
    "remove_attributes.append('HRSAWAY')\n",
    "#Hours away from work, part-week absence only low variance has 79 perecent 0s\n",
    "remove_attributes.append('PAIDOT')\n",
    "#Paid over time has low variance 83% of zeros\n",
    "remove_attributes.append('UNPAIDOT')\n",
    "#Upaid overtime has 83% of zeros\n",
    "remove_attributes.append('XTRAHRS')\n",
    "#Contains over 70% of zeroes\n",
    "\n",
    "data2=data.drop(columns=remove_attributes)\n",
    "#dropping the variables with more than 56% missing values and creating a new dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['SEX']=data2['SEX'].map({1:0,2:1})\n",
    "#Male is 0 and Female is 1. \n",
    "data2['IMMIG']=data2['IMMIG'].map({1:1,2:1,3:0})\n",
    "#immigrant is 1 and non immigrant is 0data2['MARSTAT']=data2['MARSTAT'].map({1:1,2:0,3:0,4:0,5:0,6:0})\n",
    "data2['MARSTAT']=data2['MARSTAT'].map({1:1,2:0,3:0,4:0,5:0,6:0})\n",
    "#Married is 1 and not married is 0\n",
    "data2['CMA']=data2['CMA'].map({0:0,1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:1})\n",
    "#1 is for the 9 largest CMAs and 0 is for the rest.\n",
    "\n",
    "data2['EFAMTYPE']=data2['EFAMTYPE'].map({\n",
    "    1:'Person not in an economic family',\n",
    "    2:'Dual-earner couple, no children or none under 25',\n",
    "    3:'Dual-earner couple, youngest child 0 to 17',\n",
    "    4:'Dual-earner couple, youngest child 18 to 24',\n",
    "    5:'Single-earner couple, male employed, no children or none under 25',\n",
    "    6:'Single-earner couple, male employed, youngest child 0 to 17',\n",
    "    7:'Single-earner couple, male employed, youngest child 18 to 24',\n",
    "    8:'Single-earner couple, female employed, no children or none under 25',\n",
    "    9:'Single-earner couple, female employed, youngest child 0 to 17',\n",
    "    10:'Single-earner couple, female employed, youngest child 18 to 24',\n",
    "    11:'Non-earner couple, no children or none under 25',\n",
    "    12:'Non-earner couple, youngest child 0 to 17',\n",
    "    13:'Non-earner couple, youngest child 18 to 24',\n",
    "    14:'Lone-parent family, parent employed, youngest child 0 to 17',\n",
    "    15:'Lone-parent family, parent employed, youngest child 18 to 24',\n",
    "    16:'Lone-parent family, parent not employed, youngest child 0 to 17',\n",
    "    17:'Lone-parent family, parent not employed, youngest child 18 to 24',\n",
    "    18:'Other families'\n",
    "})\n",
    "# Mapping the economic family type\n",
    "\n",
    "data2['COWMAIN']=data2['COWMAIN'].map({1:'Public sector employees',2:'Private sector employees'})\n",
    "# Mapping the class of worker\n",
    "\n",
    "data2['PROV']=data2['PROV'].map({10:'NL',11:'PE',12:'NS',13:'NB',24:'QC',35:'ON',46:'MB',47:'SK',48:'AB',59:'BC'})\n",
    "# Mapping the provinces\n",
    "\n",
    "data2['AGE_12']=data2['AGE_12'].map({\n",
    "    1:'15-19'\n",
    "    ,2:'20-24'\n",
    "    ,3:'25-29'\n",
    "    ,4:'30-34'\n",
    "    ,5:'35-39'\n",
    "    ,6:'40-44'\n",
    "    ,7:'45-49'\n",
    "    ,8:'50-54'\n",
    "    ,9:'55-59'\n",
    "    ,10:'60-64'\n",
    "    ,11:'65-69'\n",
    "    ,12:'70+'})\n",
    "# Mapping the age groups\n",
    "\n",
    "data2['EDUC']=data2['EDUC'].map({\n",
    "    0:'0 to 8 years'\n",
    "    ,1:'Some high school'\n",
    "    ,2:'High school graduate'\n",
    "    ,3:'Some post-secondary'\n",
    "    ,4:'Post-secondary certificate or diploma'\n",
    "    ,5:'Bachelor\\'s degree'\n",
    "    ,6:'Above bachelor\\'s degree'})\n",
    "# Mapping the education levels\n",
    "\n",
    "data2['NAICS_21']=data2['NAICS_21'].map({\n",
    "1:'Agriculture'\n",
    ",2:'Forestry and logging and support activities for forestry'\n",
    ",3:'Fishing, hunting and trapping'\n",
    ",4:'Mining, quarrying, and oil and gas extraction'\n",
    ",5:'Utilities'\n",
    ",6:'Construction'\n",
    ",7:'Manufacturing - durable goods'\n",
    ",8:'Manufacturing - non-durable goods'\n",
    ",9:'Wholesale trade'\n",
    ",10:'Retail trade'\n",
    ",11:'Transportation and warehousing'\n",
    ",12:'Finance and insurance'\n",
    ",13:'Real estate and rental and leasing'\n",
    ",14:'Professional, scientific and technical services'\n",
    ",15:'Business, building and other support services'\n",
    ",16:'Educational services'\n",
    ",17:'Health care and social assistance'\n",
    ",18:'Information, culture and recreation'\n",
    ",19:'Accommodation and food services'\n",
    ",20:'Other services (except public administration)'\n",
    ",21:'Public administration' \n",
    "})\n",
    "# Mapping the industry\n",
    "\n",
    "data2['FIRMSIZE']=data2['FIRMSIZE'].map({\n",
    "     1:'Less than 20 employees'\n",
    "    ,2:'20-99 employees'\n",
    "    ,3:'100-500 employees'\n",
    "    ,4:'More than 500 employees'})\n",
    "# Mapping the firm size\n",
    "\n",
    "data2['UNION']=data2['UNION'].map({\n",
    "    1:'Union member',\n",
    "    2:'Not a member but covered by a union contract or collective agreement',\n",
    "    3:'Non-unionized'})\n",
    "# Mapping the union status\n",
    "\n",
    "data2['NOC_43']=data2['NOC_43'].map({\n",
    "    1:'Legislative and senior management occupations',\n",
    "    2:'Specialized middle management occupations',\n",
    "    3:'Middle management occupations in retail and wholesale trade and customer services',\n",
    "    4:'Middle management occupations in trades, transportation, production and utilities',\n",
    "    5:'Professional occupations in finance',\n",
    "    6:'Professional occupations in business',\n",
    "    7:'Administrative and financial supervisors and specialized administrative occupations',\n",
    "    8:'Administrative occupations and transportation logistics occupations',\n",
    "    9:'Administrative and financial support and supply chain logistics occupations',\n",
    "    10:'Professional occupations in natural sciences',\n",
    "    11:'Professional occupations in applied sciences (except engineering)',\n",
    "    12:'Professional occupations in engineering',\n",
    "    13:'Technical occupations related to natural and applied sciences',\n",
    "    14:'Health treating and consultation services professionals',\n",
    "    15:'Therapy and assessment professionals',\n",
    "    16:'Nursing and allied health professionals',\n",
    "    17:'Technical occupations in health',\n",
    "    18:'Assisting occupations in support of health services',\n",
    "    19:'Professional occupations in law',\n",
    "    20:'Professional occupations in education services',\n",
    "    21:'Professional occupations in social and community services',\n",
    "    22:'Professional occupations in government services',\n",
    "    23:'Occupations in front-line public protection services',\n",
    "    24:'Paraprofessional occupations in legal, social, community and education services',\n",
    "    25:'Assisting occupations in education and in legal and public protection',\n",
    "    26:'Care providers and public protection support occupations and student monitors, crossing guards and related occupations',\n",
    "    27:'Professional occupations in art and culture',\n",
    "    28:'Technical occupations in art, culture and sport',\n",
    "    29:'Occupations in art, culture and sport',\n",
    "    30:'Support occupations in art, culture and sport',\n",
    "    31:'Retail sales and service supervisors and specialized occupations in sales and services',\n",
    "    32:'Occupations in sales and services',\n",
    "    33:'Sales and service representatives and other customer and personal services occupations',\n",
    "    34:'Sales and service support occupations',\n",
    "    35:'Technical trades and transportation officers and controllers',\n",
    "    36:'General trades',\n",
    "    37:'Mail and message distribution, other transport equipment operators and related maintenance workers',\n",
    "    38:'Helpers and labourers and other transport drivers, operators and labourers',\n",
    "    39:'Supervisors and occupations in natural resources, agriculture and related production',\n",
    "    40:'Workers and labourers in natural resources, agriculture and related production',\n",
    "    41:'Supervisors, central control and process operators in processing, manufacturing and utilities and aircraft assemblers and inspectors',\n",
    "    42:'Machine operators, assemblers and inspectors in processing, manufacturing and printing',\n",
    "    43:'Labourers in processing, manufacturing and utilities'\n",
    "})\n",
    "# Mapping the occupation\n",
    "\n",
    "data2['NOC_10']=data2['NOC_10'].map({\n",
    "    1:'Management occupations',\n",
    "    2:'Business, finance and administration occupations, except management',\n",
    "    3:'Natural and applied sciences and related occupations, except management',\n",
    "    4:'Health occupations, except management',\n",
    "    5:'Occupations in education, law and social, community and government services, except management',\n",
    "    6:'Occupations in art, culture, recreation and sport, except management',\n",
    "    7:'Sales and service occupations, except management',\n",
    "    8:'Trades, transport and equipment operators and related occupations, except management',\n",
    "    9:'Natural resources, agriculture and related production occupations, except management',\n",
    "    10:'Occupations in manufacturing and utilities, except management'})\n",
    "# Mapping the occupation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile2 = ProfileReport(data2, title=\"Profiling Report\")\n",
    "#profile2.to_file(\"EDA after removing non-correlated features.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=data2[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will be dropping the remaaining measures becauase of high correlation with other variables\n",
    "remove_attributes2=[]\n",
    "# Creating an empty list to store the variables to be removed\n",
    "remove_attributes2.append('NOC_43')\n",
    "#correlated with sex\n",
    "remove_attributes2.append('NOC_10')\n",
    "#correlated with sex\n",
    "remove_attributes2.append('ESTSIZE')\n",
    "#dropping MJH because it only contains one value, so it is not useful for the model\n",
    "remove_attributes2.append('AHRSMAIN')\n",
    "#correlated with UTOTHRS, FTPTMAIN, UHRSMAIN, ATOTHRS\n",
    "remove_attributes2.append('UTOTHRS')\n",
    "#Usual hours worked per week at all jobs, only looking at single job holder\n",
    "remove_attributes2.append('ATOTHRS')\n",
    "#Actual hours worked per week at all jobs, only looking at single job holder\n",
    "remove_attributes2.append('COWMAIN')\n",
    "#removing cowmain because it is correlated with naics_21\n",
    "remove_attributes2.append('EFAMTYPE')\n",
    "#removing efamtype because it is correlated with marstat\n",
    "\n",
    "#remove_attributes2.append('UNION')\n",
    "#removing this because it is correlated tenure\n",
    "\n",
    "data3=data3.drop(columns=remove_attributes2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile3 = ProfileReport(data3, title=\"Profiling Report\")\n",
    "#profile3.to_file(\"EDA after removing the correlated features.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list_of_variables_removed=remove_attributes+remove_attributes2\n",
    "null_attributes_full_variable_name=[]\n",
    "for attribute in master_list_of_variables_removed:\n",
    "    null_attributes_full_variable_name.append(full_variable_name_mapping.get(attribute.lower()))\n",
    "\n",
    "null_attributes_table=pd.DataFrame({'Variable':master_list_of_variables_removed,'Description':null_attributes_full_variable_name})\n",
    "#null_attributes_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Create subplots\\nfig, ax = plt.subplots(ncols=3, nrows=1, figsize=(20, 7))\\n\\n# Add titles and labels\\ntitles = ['Hourly Earnings (in dollars)', 'Usual Hours Worked (per week)', 'Job Tenure (in months)']\\nx_labels = ['Dollars', 'Hours', 'Months']\\n\\n\\n# Plot each feature in a separate subplot\\nfor i, axi in enumerate(ax.flat):\\n    try:\\n        axi.hist(feature_series[i], bins=50, color='skyblue', edgecolor='black')\\n        axi.set_title(titles[i], fontsize=14)\\n        axi.set_xlabel(x_labels[i], fontsize=12)\\n        axi.set_ylabel('Frequency', fontsize=12)\\n    except IndexError:\\n        break\\n\\nplt.tight_layout()\\n#plt.show()\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature series to be plotted\n",
    "feature_series = [data3['HRLYEARN'], data3['UHRSMAIN'], data3['TENURE']]\n",
    "\n",
    "'''\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(20, 7))\n",
    "\n",
    "# Add titles and labels\n",
    "titles = ['Hourly Earnings (in dollars)', 'Usual Hours Worked (per week)', 'Job Tenure (in months)']\n",
    "x_labels = ['Dollars', 'Hours', 'Months']\n",
    "\n",
    "\n",
    "# Plot each feature in a separate subplot\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    try:\n",
    "        axi.hist(feature_series[i], bins=50, color='skyblue', edgecolor='black')\n",
    "        axi.set_title(titles[i], fontsize=14)\n",
    "        axi.set_xlabel(x_labels[i], fontsize=12)\n",
    "        axi.set_ylabel('Frequency', fontsize=12)\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistic: 0.8565079569816589, p-value: 0.0\n",
      "HRLYEARN does not look like a normal distribution (reject H0)\n",
      "Statistic: 0.6125021576881409, p-value: 0.0\n",
      "UHRSMAIN does not look like a normal distribution (reject H0)\n",
      "Statistic: 0.8704522252082825, p-value: 0.0\n",
      "TENURE does not look like a normal distribution (reject H0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hk/opt/anaconda3/lib/python3.9/site-packages/scipy/stats/_morestats.py:1816: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    }
   ],
   "source": [
    "# Perform Shapiro-Wilk test\n",
    "for column in ['HRLYEARN','UHRSMAIN','TENURE']:\n",
    "    stat, p = stats.shapiro(data3[column])\n",
    "    print(f'Statistic: {stat}, p-value: {p}')\n",
    "    # Interpret the result\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print(f'{column} looks like a normal distribution (fail to reject H0)')\n",
    "    else:\n",
    "        print(f'{column} does not look like a normal distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Customize flier properties for outliers\\nflierprops = dict(marker='o', markerfacecolor='r', markersize=12,\\n                  linestyle='none', markeredgecolor='g')\\n\\n# Create subplots\\nfig, ax = plt.subplots(ncols=3, nrows=1, figsize=(20, 7))\\n\\n# Plot each feature in a separate subplot\\n\\nfor i, axi in enumerate(ax.flat):\\n    try:\\n        axi.boxplot(feature_series[i], flierprops=flierprops, patch_artist=True,\\n                    boxprops=dict(facecolor='skyblue', color='black'),\\n                    capprops=dict(color='black'),\\n                    whiskerprops=dict(color='black'),\\n                    medianprops=dict(color='red'))\\n        axi.set_title(feature_labels[i], fontsize=14)\\n        axi.tick_params(axis='y', labelsize=12)\\n        axi.tick_params(axis='x', labelsize=0)\\n    except IndexError:\\n        break\\n\\nplt.tight_layout()\\n#plt.show()\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature series to be plotted\n",
    "feature_series = [data3['HRLYEARN'], data3['UHRSMAIN'], data3['TENURE']]\n",
    "feature_labels = ['Hourly Earnings (in dollars)', 'Usual Hours Worked (per week)', 'Job Tenure (in months)']\n",
    "\n",
    "'''\n",
    "# Customize flier properties for outliers\n",
    "flierprops = dict(marker='o', markerfacecolor='r', markersize=12,\n",
    "                  linestyle='none', markeredgecolor='g')\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(20, 7))\n",
    "\n",
    "# Plot each feature in a separate subplot\n",
    "\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    try:\n",
    "        axi.boxplot(feature_series[i], flierprops=flierprops, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='skyblue', color='black'),\n",
    "                    capprops=dict(color='black'),\n",
    "                    whiskerprops=dict(color='black'),\n",
    "                    medianprops=dict(color='red'))\n",
    "        axi.set_title(feature_labels[i], fontsize=14)\n",
    "        axi.tick_params(axis='y', labelsize=12)\n",
    "        axi.tick_params(axis='x', labelsize=0)\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 number summary for HRLYEARN.\n",
    "Q1_HRLYEARN = np.percentile(data3['HRLYEARN'], 25)\n",
    "Q2_HRLYEARN  = np.percentile(data3['HRLYEARN'], 50)\n",
    "Q3_HRLYEARN  = np.percentile(data3['HRLYEARN'], 75)\n",
    "min_value_HRLYEARN  = np.min(data3['HRLYEARN'])\n",
    "max_value_HRLYEARN  = np.max(data3['HRLYEARN'])\n",
    "IQR_HRLYEARN = Q3_HRLYEARN  - Q1_HRLYEARN \n",
    "#print(\"Five Number Summary Q1,Q2,Q3,min_value,max_value, and IQR:\",Q1_HRLYEARN ,Q2_HRLYEARN ,Q3_HRLYEARN , min_value_HRLYEARN ,max_value_HRLYEARN , IQR_HRLYEARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_UHRSMAIN = np.percentile(data2['UHRSMAIN'], 25)\n",
    "Q2_UHRSMAIN = np.percentile(data2['UHRSMAIN'], 50)\n",
    "Q3_UHRSMAIN = np.percentile(data2['UHRSMAIN'], 75)\n",
    "min_value_UHRSMAIN = np.min(data2['UHRSMAIN'])\n",
    "max_value_UHRSMAIN = np.max(data2['UHRSMAIN'])\n",
    "IQR_UHRSMAIN = Q3_UHRSMAIN - Q1_UHRSMAIN\n",
    "#print(\"Five Number Summary Q1,Q2,Q3,min_value,max_value, and IQR:\",Q1_UHRSMAIN,Q2_UHRSMAIN,Q3_UHRSMAIN, min_value_UHRSMAIN,max_value_UHRSMAIN, IQR_UHRSMAIN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_TENURE = np.percentile(data2['TENURE'], 25)\n",
    "Q2_TENURE = np.percentile(data2['TENURE'], 50)\n",
    "Q3_TENURE = np.percentile(data2['TENURE'], 75)\n",
    "min_value_TENURE = np.min(data2['TENURE'])\n",
    "max_value_TENURE = np.max(data2['TENURE'])\n",
    "IQR_TENURE = Q3_TENURE - Q1_TENURE\n",
    "#print(\"Five Number Summary Q1,Q2,Q3,min_value,max_value, and IQR:\",Q1_TENURE,Q2_TENURE,Q3_TENURE, min_value_TENURE,max_value_TENURE, IQR_TENURE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for outliers in the response variable using IQR\n",
    "outliers_HRLYEARN = data3[(data2['HRLYEARN'] < (Q1_HRLYEARN - 1.5 * IQR_HRLYEARN)) | (data3['HRLYEARN'] > (Q3_HRLYEARN + 1.5 * IQR_HRLYEARN))]\n",
    "outliers_UHRSMAIN = data3[(data3['UHRSMAIN'] < (Q1_UHRSMAIN - 1.5 * IQR_UHRSMAIN)) | (data3['UHRSMAIN'] > (Q3_UHRSMAIN + 1.5 * IQR_UHRSMAIN))]\n",
    "outliers_TENURE = data3[(data3['TENURE'] < (Q1_TENURE - 1.5 * IQR_TENURE)) | (data3['TENURE'] > (Q3_TENURE + 1.5 * IQR_TENURE))]\n",
    "#print(\"The number of outliers for HRLYEARN, UHRSMAIN, and TENURE are:\",len(outliers_HRLYEARN),len(outliers_UHRSMAIN),len(outliers_TENURE))\n",
    "#print(\"The shape of the working dataset is:\",data3.shape)\n",
    "#The outliers account for 14.83% of the data. The outliers will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dataframe with the all the unique values in data3 and the number of classes in the categorical columns\n",
    "unique_values_table=pd.DataFrame()\n",
    "for column in data3.columns:\n",
    "    unique_values_table[column]=[data3[column].nunique()]\n",
    "unique_values_table=unique_values_table.T\n",
    "unique_values_table=unique_values_table.rename(columns={0:'Unique Values'})\n",
    "unique_values_table=unique_values_table.reset_index()\n",
    "unique_values_table=unique_values_table.rename(columns={'index':'Variables'})\n",
    "unique_values_table=unique_values_table.sort_values(by='Unique Values',ascending=False)\n",
    "unique_values_table.reset_index(drop=True,inplace=True)\n",
    "#unique_values_table.to_excel('Unique Values Table_Figure_4.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4=data3[:]\n",
    "# Crating a newdataframe to remove the outliers from data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the outliers from the dataset\n",
    "data4 = data4[(data4['HRLYEARN'] >= (Q1_HRLYEARN - 1.5 * IQR_HRLYEARN)) & (data4['HRLYEARN'] <= (Q3_HRLYEARN + 1.5 * IQR_HRLYEARN))]\n",
    "data4 = data4[(data4['UHRSMAIN'] >= (Q1_UHRSMAIN - 1.5 * IQR_UHRSMAIN)) & (data4['UHRSMAIN'] <= (Q3_UHRSMAIN + 1.5 * IQR_UHRSMAIN))]\n",
    "data4 = data4[(data4['TENURE'] >= (Q1_TENURE - 1.5 * IQR_TENURE)) & (data4['TENURE'] <= (Q3_TENURE + 1.5 * IQR_TENURE))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"The median of the response variable before and after removing the outliers:\", data3[\"HRLYEARN\"].median(),data4[\"HRLYEARN\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data4[\"HRLYEARN\"]  \n",
    "# Hypothetical median\n",
    "hypothetical_median = 33.55 #Population median\n",
    "differences = sample_data - hypothetical_median\n",
    "\n",
    "# Perform one-sample Wilcoxon signed-rank test\n",
    "statistic, p_value = stats.wilcoxon(differences)\n",
    "#print(f\"Wilcoxon signed-rank test: statistic = {statistic}, p-value = {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_statistics_table_continuous_variables_before_outliers=statistics_table(data3[['HRLYEARN','UHRSMAIN','TENURE']])\n",
    "summary_statistics_table_continuous_variables_after_outliers=statistics_table(data4[['HRLYEARN','UHRSMAIN','TENURE']])\n",
    "# Creating the summary statistics table for the continuous variables before and after removing the outliers\n",
    "\n",
    "#summary_statistics_table_continuous_variables_before_outliers.to_excel('Continuous Variables Before Removing outliers.xlsx',index=False)  \n",
    "#summary_statistics_table_continuous_variables_after_outliers.to_excel('Continuous Variables After Removing outliers.xlsx',index=False)  \n",
    "# Exporting the summary statistics table for the continuous variables before and after removing the outliers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5=data4[:]\n",
    "# Creating new data frame after removing the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5.rename(columns={'AGE_12': 'AGE'}, inplace=True)\n",
    "# Renaming the age column\n",
    "data5.rename(columns={'NAICS_21':'NAICS'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of transformed data: (127821, 55)\n",
      "Number of feature names: 55\n",
      "DataFrame created successfully with transformed features.\n",
      "    UHRSMAIN    TENURE  PROV_BC  PROV_MB  PROV_NB  PROV_NL  PROV_NS  PROV_ON  \\\n",
      "6   0.373737  1.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "7   0.626263  0.066946      1.0      0.0      0.0      0.0      0.0      0.0   \n",
      "10  0.121212  1.000000      1.0      0.0      0.0      0.0      0.0      0.0   \n",
      "11  0.626263  0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "16  0.373737  0.121339      0.0      0.0      0.0      0.0      0.0      1.0   \n",
      "\n",
      "    PROV_PE  PROV_QC  ...  NAICS_Real estate and rental and leasing  \\\n",
      "6       0.0      1.0  ...                                       0.0   \n",
      "7       0.0      0.0  ...                                       0.0   \n",
      "10      0.0      0.0  ...                                       0.0   \n",
      "11      0.0      1.0  ...                                       0.0   \n",
      "16      0.0      0.0  ...                                       0.0   \n",
      "\n",
      "    NAICS_Retail trade  NAICS_Transportation and warehousing  NAICS_Utilities  \\\n",
      "6                  0.0                                   0.0              0.0   \n",
      "7                  0.0                                   0.0              0.0   \n",
      "10                 0.0                                   0.0              0.0   \n",
      "11                 0.0                                   0.0              0.0   \n",
      "16                 0.0                                   0.0              0.0   \n",
      "\n",
      "    NAICS_Wholesale trade  \\\n",
      "6                     0.0   \n",
      "7                     0.0   \n",
      "10                    0.0   \n",
      "11                    0.0   \n",
      "16                    0.0   \n",
      "\n",
      "    UNION_Not a member but covered by a union contract or collective agreement  \\\n",
      "6                                                 0.0                            \n",
      "7                                                 0.0                            \n",
      "10                                                0.0                            \n",
      "11                                                0.0                            \n",
      "16                                                0.0                            \n",
      "\n",
      "    UNION_Union member  FIRMSIZE_20-99 employees  \\\n",
      "6                  0.0                       0.0   \n",
      "7                  0.0                       1.0   \n",
      "10                 1.0                       0.0   \n",
      "11                 0.0                       0.0   \n",
      "16                 1.0                       0.0   \n",
      "\n",
      "    FIRMSIZE_Less than 20 employees  FIRMSIZE_More than 500 employees  \n",
      "6                               0.0                               1.0  \n",
      "7                               0.0                               0.0  \n",
      "10                              0.0                               1.0  \n",
      "11                              0.0                               1.0  \n",
      "16                              0.0                               0.0  \n",
      "\n",
      "[5 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "categorical_features = ['PROV', 'CMA', 'AGE', 'SEX', 'MARSTAT', 'EDUC', 'IMMIG', 'NAICS', 'UNION', 'FIRMSIZE']\n",
    "continuous_features = ['UHRSMAIN', 'TENURE']\n",
    "\n",
    "# Define the response variable\n",
    "y = data5['HRLYEARN']\n",
    "\n",
    "# Define the feature variables\n",
    "X = data5.drop(columns=['HRLYEARN'])\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), continuous_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Check the shape of the transformed data\n",
    "print(\"Shape of transformed data:\", X_transformed.shape)\n",
    "\n",
    "# Get feature names after transformation\n",
    "categorical_transformer = preprocessor.named_transformers_['cat']\n",
    "onehot_feature_names = categorical_transformer.get_feature_names_out(categorical_features)\n",
    "feature_names = np.append(continuous_features, onehot_feature_names)\n",
    "\n",
    "# Check the length of feature names\n",
    "print(\"Number of feature names:\", len(feature_names))\n",
    "\n",
    "# Convert sparse matrix to dense matrix\n",
    "X_transformed_dense = X_transformed.toarray()\n",
    "\n",
    "# Create a DataFrame with transformed features and set the original index\n",
    "try:\n",
    "    X_transformed_df = pd.DataFrame(X_transformed_dense, columns=feature_names, index=data5.index)\n",
    "    print(\"DataFrame created successfully with transformed features.\")\n",
    "    print(X_transformed_df.head())\n",
    "except ValueError as e:\n",
    "    print(\"Error creating DataFrame:\", e)\n",
    "    print(\"Number of columns in transformed data:\", X_transformed_dense.shape[1])\n",
    "    print(\"Number of feature names:\", len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'model__n_estimators': [10],\n",
    "    'model__max_features': [1.0],\n",
    "    'model__max_depth': [None],\n",
    "    'model__min_samples_split': [2],\n",
    "    'model__min_samples_leaf': [1],\n",
    "    'model__bootstrap': [True]\n",
    "}\n",
    "# Define the KFold cross-validator\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid,n_iter=1,cv=kf,random_state=42, n_jobs=-1)\n",
    "\n",
    "# Can only do 1 iteration because of computaional constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the RandomizedSearchCV\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Get the best model from RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "#print(f\"Best hyperparameters: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Plot feature importances\\nplt.figure(figsize=(12, 8))\\nsns.barplot(x='Importance', y='Feature', data=features_df, palette='viridis')\\nplt.title('Feature Importances from Random Forest')\\nplt.xlabel('Importance')\\nplt.ylabel('Feature')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retreived the best model after fitting the RandomizedSearchCV\n",
    "model = best_model.named_steps['model']\n",
    "\n",
    "# Retreiving the fature names\n",
    "categorical_transformer = best_model.named_steps['preprocessor'].named_transformers_['cat']\n",
    "onehot_feature_names = categorical_transformer.get_feature_names_out(categorical_features)\n",
    "feature_names = np.append(continuous_features, onehot_feature_names)\n",
    "\n",
    "# Extracted feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Created a DataFrame for better visualization\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sorted the DataFrame by importance\n",
    "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "# Kept 95% of the cumulative importance\n",
    "\n",
    "# Calculate cumulative importance\n",
    "features_df['Cumulative Importance'] = features_df['Importance'].cumsum()\n",
    "\n",
    "#features_df.to_excel('Feature Importance Table.xlsx',index=False)\n",
    "# Exporting the feature importance table\n",
    "\n",
    "\n",
    "# Display the DataFrame with importance\n",
    "#print(features_df)\n",
    "'''\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=features_df, palette='viridis')\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the cumulative importance threshold\n",
    "threshold = 0.95\n",
    "\n",
    "# Identify the features to keep\n",
    "features_to_keep = features_df[features_df['Cumulative Importance'] <= threshold]['Feature']\n",
    "\n",
    "#print(f\"Number of features to keep: {len(features_to_keep)}\")\n",
    "#print(f\"Features to keep:\\n{features_to_keep}\")\n",
    "\n",
    "# Subset the transformed dataset to include only the top 10 features\n",
    "X_reduced = X_transformed_df[features_to_keep]\n",
    "#print(\"Reduced dataset shape:\", X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_reduced.shape)\n",
    "#print(data5['HRLYEARN'].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is after the linearity assumption is done\n",
    "preparing_features_for_models=X_reduced.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure indices are aligned\n",
    "X_reduced = X_reduced.reset_index(drop=True)\n",
    "y = data5['HRLYEARN'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127821, 41)\n",
      "(127821,)\n"
     ]
    }
   ],
   "source": [
    "print(X_reduced.shape)\n",
    "print(y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "# Define the independent variables (X)\n",
    "X = X_reduced\n",
    "\n",
    "# Add a constant to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "model_summary = model.summary()\n",
    "#print(model_summary)\n",
    "\n",
    "# Get the residuals\n",
    "residuals = model.resid\n",
    "\n",
    "# 1. Check for independence (Durbin-Watson test)\n",
    "dw_test = sm.stats.stattools.durbin_watson(residuals)\n",
    "#print('Durbin-Watson test statistic:', dw_test)\n",
    "\n",
    "# 2. Check for normality (Q-Q plot and Shapiro-Wilk test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "shapiro_test = stats.shapiro(residuals)\n",
    "#print('Shapiro-Wilk test statistic:', shapiro_test)\n",
    "\n",
    "# 3. Check for mean of residuals equal to zero\n",
    "mean_residuals = residuals.mean()\n",
    "#print('Mean of residuals:', mean_residuals)\n",
    "\n",
    "# 4. Check for homoscedasticity (constant variance)\n",
    "plt.scatter(model.fittedvalues, residuals)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Fitted values vs Residuals')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Apply log transformation to the dependent variable to fix the normality issue and check the residuals again\n",
    "# This fixed the normaility issue but not the homoscedasticity issue\n",
    " \n",
    "#transformed dependent variable (y)\n",
    "log_y=np.log(y)\n",
    "\n",
    "# Add a constant to the model\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(log_y, X).fit()\n",
    "\n",
    "# Get the residuals\n",
    "residuals = model.resid\n",
    "\n",
    "# 1. Check for independence (Durbin-Watson test)\n",
    "dw_test = sm.stats.stattools.durbin_watson(residuals)\n",
    "#print('Durbin-Watson test statistic:', dw_test)\n",
    "\n",
    "# 2. Check for normality (Q-Q plot and Shapiro-Wilk test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(residuals, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "shapiro_test = stats.shapiro(residuals)\n",
    "#print('Shapiro-Wilk test statistic:', shapiro_test)\n",
    "\n",
    "# 3. Check for mean of residuals equal to zero\n",
    "mean_residuals = residuals.mean()\n",
    "#print('Mean of residuals:', mean_residuals)\n",
    "\n",
    "# 4. Check for homoscedasticity (constant variance)\n",
    "plt.scatter(model.fittedvalues, residuals)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Fitted values vs Residuals')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Useing weighted least squares (WLS) to account for heteroscedasticity, but does not workw well.\n",
    "model = sm.OLS(y, X).fit()\n",
    "weights = 1 / model.fittedvalues\n",
    "model_wls = sm.WLS(y, X, weights=weights).fit()\n",
    "\n",
    "# Check residuals for homoscedasticity\n",
    "residuals_wls = model_wls.resid\n",
    "plt.scatter(model_wls.fittedvalues, residuals_wls)\n",
    "plt.xlabel('Fitted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Fitted values vs Residuals (WLS)')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127821, 41)\n",
      "(127821, 2)\n"
     ]
    }
   ],
   "source": [
    "features= preparing_features_for_models.copy()\n",
    "features.drop(columns=['TENURE','UHRSMAIN'],inplace=True)\n",
    "#dropping these features because they are scaled, and we need to keep the original values for the non-linear models, and scaled later.\n",
    "continuous_features=data5[['TENURE', 'UHRSMAIN']]\n",
    "\n",
    "#features.to_excel('Features .xlsx',index=True)\n",
    "#continuous_features.to_excel('Continuous Features.xlsx',index=True)\n",
    "#data5['HRLYEARN'].to_excel('Response Variable.xlsx',index=True)\n",
    "\n",
    "#features=features.reset_index(drop=True)\n",
    "#continuous_features=continuous_features.reset_index(drop=True)\n",
    "\n",
    "#data5.to_excel('Data5.xlsx',index=True)\n",
    "\n",
    "features=features.merge(continuous_features,left_index=True,right_index=True)\n",
    "\n",
    "print(features.shape)\n",
    "print(continuous_features.shape)\n",
    "\n",
    "label = data5['HRLYEARN']\n",
    "#These will be used for the non-linear machine learning algoirthms. \n",
    "label=label.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_the_feautres_without_rf=X_transformed_df.copy()\n",
    "#This has all the feautures, not dropping anything\n",
    "\n",
    "all_the_feautres_without_rf.drop(columns=['TENURE','UHRSMAIN'],inplace=True)\n",
    "#dropping these features because they are scaled, and we need to keep the original values for the non-linear models, and scaled later.\n",
    "\n",
    "all_the_feautres_without_rf=all_the_feautres_without_rf.merge(continuous_features,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features=features.copy()\n",
    "#This is dropping the imbalances detected after hot ones encoding \n",
    "\n",
    "top_features.drop(columns=[\n",
    "'NAICS_Public administration',\n",
    "'NAICS_Mining, quarrying, and oil and gas extraction',\n",
    "'NAICS_Professional, scientific and technical services',\n",
    "'NAICS_Retail trade',\n",
    "'AGE_60-64',\n",
    "'PROV_MB',\n",
    "'NAICS_Finance and insurance',\n",
    "'AGE_25-29',\n",
    "'PROV_SK',\n",
    "'NAICS_Construction',\n",
    "'NAICS_Utilities',\n",
    "'PROV_NS',\n",
    "'NAICS_Manufacturing - non-durable goods',\n",
    "'NAICS_Wholesale trade',\n",
    "'PROV_NB',\n",
    "'AGE_20-24',\n",
    "'NAICS_Manufacturing - durable goods',\n",
    "],axis=1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "profile4 = ProfileReport(top_features_and_label_for_profile_report, title=\"Profiling Report\")\n",
    "profile4.to_file(\"EDA of the top features.html\")\n",
    "#Creating the final profile report\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the DataFrame and the Series\n",
    "data_aprori = features.copy()\n",
    "data_aprori.drop(columns=['TENURE','UHRSMAIN'],inplace=True)\n",
    "#Need to drop TENURE and UHRSMAIN because they scaled and transformed, but will need the original values in order to classify them below into 5 buckets.\n",
    "data_aprori.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data=data4[['TENURE','UHRSMAIN','HRLYEARN']]\n",
    "numeric_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical features to be discretized\n",
    "numerical_variables = ['UHRSMAIN', 'TENURE','HRLYEARN']\n",
    "# Initialize the KBinsDiscretizer with 'uniform' strategy for equal-width binning\n",
    "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "# Fit and transform the data\n",
    "numeric_data[numerical_variables] = discretizer.fit_transform(numeric_data[numerical_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to one-hot encode\n",
    "to_encode = numeric_data[numerical_variables]\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "encoded_features = encoder.fit_transform(to_encode)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "encoded_feature_names = encoder.get_feature_names_out(numerical_variables)\n",
    "\n",
    "# Convert the result back to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names)\n",
    "\n",
    "#print(\"One-Hot Encoded DataFrame:\\n\", encoded_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aprori = pd.concat([data_aprori, encoded_df], axis=1)\n",
    "#add back the one hot encoded variables to the data_aprori dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aprori.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bin edges\n",
    "bin_edges = discretizer.bin_edges_\n",
    "\n",
    "# Create a dictionary to map feature names to their bin edges\n",
    "bin_edges_dict = {feature: edges for feature, edges in zip(numerical_variables, bin_edges)}\n",
    "\n",
    "# Print bin edges for each feature\n",
    "for feature, edges in bin_edges_dict.items():\n",
    "    print(f\"Bin edges for {feature}: {edges}\")\n",
    "\n",
    "    # Prepare data for DataFrame\n",
    "bin_info = []\n",
    "\n",
    "for feature, edges in bin_edges_dict.items():\n",
    "    for i in range(len(edges) - 1):\n",
    "        bin_info.append({\n",
    "            'Feature': feature,\n",
    "            'Bin Index': i,\n",
    "            'Bin Start': edges[i],\n",
    "            'Bin End': edges[i + 1]\n",
    "        })\n",
    "\n",
    "# Create DataFrame from bin information\n",
    "bin_info_df = pd.DataFrame(bin_info)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(bin_info_df)\n",
    "\n",
    "#bin_info_df.to_excel('Binning Information Table.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Apriori algorithm\n",
    "frequent_itemsets = apriori(data_aprori, min_support=0.1, use_colnames=True)\n",
    "print(\"Frequent itemsets:\\n\", frequent_itemsets.head())\n",
    "\n",
    "# Extract association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "#rint(\"Association rules:\\n\", rules.head())\n",
    "\n",
    "# Define the desired RHS items\n",
    "desired_rhs = ['HRLYEARN_0.0', 'HRLYEARN_1.0', 'HRLYEARN_2.0', 'HRLYEARN_3.0', 'HRLYEARN_4.0']\n",
    "\n",
    "# Filter rules by RHS\n",
    "filtered_rules = rules[rules['consequents'].apply(lambda x: any(item in x for item in desired_rhs))]\n",
    "\n",
    "print(\"Filtered association rules:\\n\", filtered_rules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a report with the important rules\n",
    "report = filtered_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]\n",
    "\n",
    "# Save the report to a CSV file\n",
    "#report.to_csv('important_association_rules_hrlyearn.csv', index=False)\n",
    "\n",
    "print(\"Report generated: important_association_rules_hrlyearn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = ['UHRSMAIN', 'TENURE']\n",
    "#Defining the continuous features\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), continuous_features)\n",
    "    ])\n",
    "#Setting up the column transformer\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "#Initializing K-Folds\n",
    "\n",
    "param_grids = {\n",
    "    'XGB': {\n",
    "        'model__n_estimators': [1000],\n",
    "        'model__learning_rate': [0.05],\n",
    "        'model__max_depth': [3, 4, 5],\n",
    "        'model__subsample': [0.7, 0.8, 0.9],\n",
    "        'model__colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'model__gamma': [0, 1, 5],\n",
    "        'model__reg_alpha': [0, 1, 5]\n",
    "    },\n",
    "    #'DT': {\n",
    "    #    'model__criterion': ['mse'],\n",
    "    #    'model__splitter': ['best', 'random'],\n",
    "    #    'model__max_depth': [3, 4, 5],\n",
    "    #    'model__min_samples_split': [2, 3, 4],\n",
    "    #    'model__min_samples_leaf': [1, 2, 3]\n",
    "    #},\n",
    "    'RF': {\n",
    "        'model__n_estimators': [1000],\n",
    "        'model__max_depth': [3, 4, 5],\n",
    "        'model__min_samples_split': [2, 3, 4],\n",
    "        'model__min_samples_leaf': [1, 2, 3]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'model__degree': [2, 3, 4, 5],\n",
    "        'model__gamma': ['scale', 'auto'],\n",
    "        'model__C': [0.1, 1, 10, 100, 1000]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model__n_neighbors': [3, 5, 7, 9],\n",
    "        'model__weights': ['uniform', 'distance'],\n",
    "        'model__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'model__p': [1, 2]\n",
    "    }\n",
    "}\n",
    "#Defining the parameter grids for each model\n",
    "\n",
    "\n",
    "models = {\n",
    "    'XGB': XGBRegressor()\n",
    "    ,\n",
    "    #'DT': DecisionTreeRegressor(random_state=42),\n",
    "    'RF': RandomForestRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'KNN': KNeighborsRegressor()\n",
    "}\n",
    "# Defining each model\n",
    "\n",
    "# Perform RandomizedSearchCV for each model\n",
    "scores = []\n",
    "#creating an empty list to store the scores\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    grid_search = RandomizedSearchCV(pipeline, param_distributions=param_grids[model_name], n_iter=1, cv=kf, random_state=42, scoring='r2')\n",
    "    grid_result = grid_search.fit(features, label)\n",
    "    \n",
    "    scores.append({\n",
    "        'model': model_name,\n",
    "        'best_score': grid_result.best_score_,\n",
    "        'best_params': grid_result.best_params_\n",
    "    })\n",
    "#Running the randomized search for each model\n",
    "\n",
    "scores_df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
    "# Creating a datafreame for all the scores\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Example: Assuming features and label are already defined\n",
    "categorical_features = ['PROV', 'CMA', 'AGE', 'SEX', 'MARSTAT', 'EDUC', 'IMMIG', 'NAICS', 'UNION', 'FIRMSIZE']\n",
    "continuous_features = ['UHRSMAIN', 'TENURE']\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), continuous_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = preprocessor.fit_transform(features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
    "X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Define the ANN model\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "    return model\n",
    "\n",
    "# Get the input dimension\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Build the model\n",
    "model = build_ann(input_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Mean Squared Error on test data: {mse}')\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R-squared: {r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
